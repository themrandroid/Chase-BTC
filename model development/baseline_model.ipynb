{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79455365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537a5146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved labeled dataset to data/features\\BTC-USD_daily_labeled.parquet\n",
      "Shape: (3709, 65), Positive ratio: 0.47\n"
     ]
    }
   ],
   "source": [
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_ml_ready.parquet\")\n",
    "OUTPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "\n",
    "def add_binary_target(df, horizon=1, neutral_threshold=None):\n",
    "    \"\"\"\n",
    "    Add classification target:\n",
    "      1 = Up, 0 = Down (optionally, 2 = Neutral if threshold given)\n",
    "    horizon: how many days ahead to predict (default = 1 day)\n",
    "    neutral_threshold: % change threshold to classify as Neutral (optional)\n",
    "    \"\"\"\n",
    "    # % change from today to horizon days ahead\n",
    "    df[\"future_return\"] = df[\"close\"].shift(-horizon) / df[\"close\"] - 1.0\n",
    "\n",
    "    if neutral_threshold is None:\n",
    "        # Binary classification\n",
    "        df[\"target\"] = (df[\"future_return\"] > 0).astype(int)\n",
    "    else:\n",
    "        # Ternary classification: Up / Down / Neutral\n",
    "        df[\"target\"] = 0  # default Down\n",
    "        df.loc[df[\"future_return\"] > neutral_threshold, \"target\"] = 1  # Up\n",
    "        df.loc[df[\"future_return\"].between(-neutral_threshold, neutral_threshold), \"target\"] = 2  # Neutral\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_labels():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(\"Run data_pipeline first to generate ML-ready features\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "    # Add labels\n",
    "    df = add_binary_target(df, horizon=1)  # Predict next-day movement\n",
    "\n",
    "    # Drop last row (no label possible at end)\n",
    "    df = df.dropna(subset=[\"future_return\", \"target\"]).reset_index(drop=True)\n",
    "\n",
    "    # Save\n",
    "    df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"Saved labeled dataset to {OUTPUT_FILE}\")\n",
    "    print(f\"Shape: {df.shape}, Positive ratio: {df['target'].mean():.2f}\")\n",
    "\n",
    "make_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3649, 60, 62), y shape: (3649,), Positive ratio: 0.47\n",
      "Batch X shape: (32, 60, 62), Batch y shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "SEQ_LEN = 60  # length of input sequence (days)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def build_sequences(df, seq_len=20, target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Build X, y arrays from a dataframe for sequence models.\n",
    "    X shape: (num_samples, seq_len, num_features)\n",
    "    y shape: (num_samples,)\n",
    "    \"\"\"\n",
    "    values = df.drop(columns=[\"timestamp\", \"future_return\", target_col]).values\n",
    "    targets = df[target_col].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(targets[i+seq_len])  # label corresponds to the day after the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def make_tf_dataset(X, y, batch_size=32, shuffle=True):\n",
    "    \"\"\"Convert numpy arrays to tf.data.Dataset.\"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X))\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_sequences():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(\"Run make_labels first to generate labeled dataset\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "    # Build sequences\n",
    "    X, y = build_sequences(df, seq_len=SEQ_LEN)\n",
    "\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}, Positive ratio: {y.mean():.2f}\")\n",
    "\n",
    "    # Build TensorFlow Dataset\n",
    "    ds = make_tf_dataset(X, y, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Example: iterate a batch\n",
    "    for xb, yb in ds.take(1):\n",
    "        print(f\"Batch X shape: {xb.shape}, Batch y shape: {yb.shape}\")\n",
    "\n",
    "make_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5f8b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Baseline Results\n",
      "Accuracy   : 0.4798\n",
      "Precision  : 0.4798\n",
      "Recall     : 1.0000\n",
      "F1 Score   : 0.6485\n",
      "ROC-AUC    : 0.5140\n"
     ]
    }
   ],
   "source": [
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "\n",
    "def baseline_log_reg():\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(\"Run make_labels first to generate labeled dataset\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "\n",
    "    look_ahead = 3  # days\n",
    "    threshold = 0.01  # +1% move\n",
    "\n",
    "    df['future_return'] = df['close'].shift(-look_ahead) / df['close'] - 1\n",
    "    df['target'] = (df['future_return'] > threshold).astype(int)\n",
    "\n",
    "\n",
    "    # Drop non-feature columns\n",
    "    X = df.drop(columns=[\"timestamp\", \"future_return\", \"target\"]).values\n",
    "    y = df[\"target\"].values\n",
    "\n",
    "    # Train/test split (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False  # no shuffle = respects time order\n",
    "    )\n",
    "\n",
    "    # Train Logistic Regression\n",
    "    clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(\"\\nLogistic Regression Baseline Results\")\n",
    "    print(f\"Accuracy   : {acc:.4f}\")\n",
    "    print(f\"Precision  : {prec:.4f}\")\n",
    "    print(f\"Recall     : {rec:.4f}\")\n",
    "    print(f\"F1 Score   : {f1:.4f}\")\n",
    "    print(f\"ROC-AUC    : {auc:.4f}\")\n",
    "\n",
    "baseline_log_reg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e827ec",
   "metadata": {},
   "source": [
    "### Building Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17fbe10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "\n",
    "\n",
    "# ----1. Build Sequences -----------\n",
    "TOP_FEATURES = ['volatility_21d', 'volatility_10d', 'return_14d', 'return_3d', 'bollinger_down']\n",
    "\n",
    "def build_sequences_filtered(df, seq_len=30, target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Build sequences but only with top selected features\n",
    "    \"\"\"\n",
    "    values = df[TOP_FEATURES].values  # Use only top features\n",
    "    targets = df[target_col].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(targets[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ----2. Get Callbacks -----------\n",
    "def get_callbacks(output_dir=\"experiments\", model_name=\"model\"):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(output_dir, f\"{model_name}_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(run_dir, \"best_model.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            filename=os.path.join(run_dir, \"training_log.csv\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks, run_dir\n",
    "\n",
    "# ----3. Walk Forward Validation -----------\n",
    "def walk_forward_validation(build_model_fn, seq_len, \n",
    "                            train_size=0.7, val_size=0.1, test_size=0.2,\n",
    "                            n_splits=3, batch_size=32, epochs=20, callbacks=None):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for time series models.\n",
    "    \n",
    "    Args:\n",
    "        X, y: numpy arrays (sequences + labels)\n",
    "        build_model_fn: function that returns a compiled tf.keras model\n",
    "        seq_len: length of input sequences\n",
    "        train_size, val_size, test_size: proportions of data\n",
    "        n_splits: number of walk-forward splits\n",
    "        batch_size, epochs: training params\n",
    "        callbacks: list of Keras callbacks\n",
    "    Returns:\n",
    "        results: list of dicts with metrics for each split\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(\"Run make_labels.py first to generate labeled dataset\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "    look_ahead = 3  # days\n",
    "    threshold = 0.01  # +1% move\n",
    "\n",
    "    df['future_return'] = df['close'].shift(-look_ahead) / df['close'] - 1\n",
    "    df['target'] = (df['future_return'] > threshold).astype(int)\n",
    "\n",
    "    # Build sequences\n",
    "    # X, y = build_sequences(df, seq_len)\n",
    "    X, y = build_sequences_filtered(df, seq_len)\n",
    "\n",
    "    results = []\n",
    "    total_len = len(X)\n",
    "    split_len = int(total_len * test_size)  # test size per split\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        # Define rolling window indices\n",
    "        end_test = total_len - i * split_len\n",
    "        start_test = end_test - split_len\n",
    "        end_train = start_test - 1\n",
    "\n",
    "        X_train, y_train = X[:end_train], y[:end_train]\n",
    "        X_test, y_test = X[start_test:end_test], y[start_test:end_test]\n",
    "\n",
    "        # Further split train into train/val\n",
    "        split_idx = int(len(X_train) * (1 - val_size))\n",
    "        X_tr, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "        y_tr, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "\n",
    "        # Build datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_tr, y_tr)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Build model\n",
    "        model = build_model_fn(seq_len, X.shape[2])\n",
    "        \n",
    "        # Compute class weights\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(train_ds, validation_data=val_ds, \n",
    "                            epochs=epochs, callbacks=callbacks, \n",
    "                            verbose=1, class_weight=class_weights)\n",
    "\n",
    "        # Evaluate\n",
    "        y_prob = model.predict(test_ds).ravel()\n",
    "        y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "        metrics = {\n",
    "            \"split\": i+1,\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "        }\n",
    "        val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
    "        metrics[\"val_loss\"] = val_loss\n",
    "        metrics[\"val_accuracy\"] = val_acc\n",
    "        metrics[\"model_path\"] = callbacks[2].filepath\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "        print(f\"\\nSplit {i+1} Results: {metrics}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29cb31",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a097fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7370 - loss: 0.5802\n",
      "Epoch 1: val_loss improved from inf to 0.70622, saving model to experiments\\lstmmedium_20250923-185045\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.7357 - loss: 0.5815 - val_accuracy: 0.4746 - val_loss: 0.7062 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7419 - loss: 0.5329\n",
      "Epoch 2: val_loss did not improve from 0.70622\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7404 - loss: 0.5344 - val_accuracy: 0.4610 - val_loss: 0.7674 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m81/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7240 - loss: 0.4879\n",
      "Epoch 3: val_loss did not improve from 0.70622\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.7215 - loss: 0.4909 - val_accuracy: 0.3932 - val_loss: 0.8244 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m81/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7487 - loss: 0.4691\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.70622\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7453 - loss: 0.4726 - val_accuracy: 0.3932 - val_loss: 0.8126 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7260 - loss: 0.4662\n",
      "Epoch 5: val_loss did not improve from 0.70622\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7243 - loss: 0.4681 - val_accuracy: 0.3932 - val_loss: 0.8259 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m81/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7230 - loss: 0.4454\n",
      "Epoch 6: val_loss did not improve from 0.70622\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7199 - loss: 0.4486 - val_accuracy: 0.3932 - val_loss: 0.8172 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\n",
      "Split 1 Results: {'split': 1, 'accuracy': 0.48027210884353744, 'precision': 0.4805013927576602, 'recall': 0.9745762711864406, 'f1': 0.6436567164179104, 'roc_auc': 0.46736213058113496, 'val_loss': 0.7062167525291443, 'val_accuracy': 0.47457626461982727, 'model_path': 'experiments\\\\lstmmedium_20250923-185045\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m62/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8027 - loss: 0.5723\n",
      "Epoch 1: val_loss improved from 0.70622 to 0.69365, saving model to experiments\\lstmmedium_20250923-185045\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.8006 - loss: 0.5740 - val_accuracy: 0.5113 - val_loss: 0.6937 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6456 - loss: 0.5558\n",
      "Epoch 2: val_loss did not improve from 0.69365\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6456 - loss: 0.5566 - val_accuracy: 0.4977 - val_loss: 0.6991 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6405 - loss: 0.5385\n",
      "Epoch 3: val_loss did not improve from 0.69365\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6404 - loss: 0.5394 - val_accuracy: 0.4977 - val_loss: 0.7205 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m62/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7085 - loss: 0.5161\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69365\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7077 - loss: 0.5181 - val_accuracy: 0.4977 - val_loss: 0.7490 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7293 - loss: 0.4856\n",
      "Epoch 5: val_loss did not improve from 0.69365\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7290 - loss: 0.4866 - val_accuracy: 0.4977 - val_loss: 0.7751 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7333 - loss: 0.4828\n",
      "Epoch 6: val_loss did not improve from 0.69365\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.7327 - loss: 0.4837 - val_accuracy: 0.4977 - val_loss: 0.7725 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\n",
      "Split 2 Results: {'split': 2, 'accuracy': 0.5197278911564626, 'precision': 0.4689655172413793, 'recall': 0.6257668711656442, 'f1': 0.5361366622864652, 'roc_auc': 0.5647621761891191, 'val_loss': 0.6936500668525696, 'val_accuracy': 0.5113122463226318, 'model_path': 'experiments\\\\lstmmedium_20250923-185045\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6411 - loss: 0.6230\n",
      "Epoch 1: val_loss improved from 0.69365 to 0.64539, saving model to experiments\\lstmmedium_20250923-185045\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.6423 - loss: 0.6230 - val_accuracy: 0.6014 - val_loss: 0.6454 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6888 - loss: 0.5709\n",
      "Epoch 2: val_loss did not improve from 0.64539\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.6883 - loss: 0.5720 - val_accuracy: 0.5000 - val_loss: 0.6821 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6972 - loss: 0.5633\n",
      "Epoch 3: val_loss improved from 0.64539 to 0.56971, saving model to experiments\\lstmmedium_20250923-185045\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6983 - loss: 0.5625 - val_accuracy: 0.6757 - val_loss: 0.5697 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7883 - loss: 0.5200\n",
      "Epoch 4: val_loss did not improve from 0.56971\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7870 - loss: 0.5206 - val_accuracy: 0.5270 - val_loss: 0.7757 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8000 - loss: 0.4697\n",
      "Epoch 5: val_loss did not improve from 0.56971\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7980 - loss: 0.4723 - val_accuracy: 0.5405 - val_loss: 0.7475 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7795 - loss: 0.4352\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.56971\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7789 - loss: 0.4374 - val_accuracy: 0.3243 - val_loss: 1.0192 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7065 - loss: 0.4897\n",
      "Epoch 7: val_loss did not improve from 0.56971\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7083 - loss: 0.4894 - val_accuracy: 0.3784 - val_loss: 0.9425 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7678 - loss: 0.3952\n",
      "Epoch 8: val_loss did not improve from 0.56971\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7683 - loss: 0.3979 - val_accuracy: 0.4527 - val_loss: 0.8490 - learning_rate: 5.0000e-04\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\n",
      "Split 3 Results: {'split': 3, 'accuracy': 0.6054421768707483, 'precision': 0.44970414201183434, 'recall': 0.59375, 'f1': 0.5117845117845118, 'roc_auc': 0.6144881915448851, 'val_loss': 0.5697149634361267, 'val_accuracy': 0.6756756901741028, 'model_path': 'experiments\\\\lstmmedium_20250923-185045\\\\best_model.h5'}\n",
      " BEST LSTM MODEL SUMMARY\n",
      "\n",
      "Best Split #: 3\n",
      "Validation Loss : 0.5697\n",
      "Validation Accuracy : 0.6757\n",
      "Test Accuracy : 0.6054\n",
      "Test F1 Score : 0.5118\n",
      "Test ROC-AUC : 0.6145\n",
      "Saved Model Path : experiments\\lstmmedium_20250923-185045\\best_model.h5\n"
     ]
    }
   ],
   "source": [
    "def build_lstm(seq_len, num_features, config=\"small\"):\n",
    "    \"\"\"\n",
    "    Build LSTM model based on config size: 'small', 'medium', 'large'\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        lstm_units = [64]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        lstm_units = [64, 32]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"large\":\n",
    "        lstm_units = [128, 64]\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.4\n",
    "\n",
    "    # Build Sequential Model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # Add LSTM layers\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_sequences = (i < len(lstm_units) - 1)  # True except last LSTM\n",
    "        model.add(tf.keras.layers.LSTM(units, return_sequences=return_sequences))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Add Dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_config = \"medium\"\n",
    "\n",
    "callbacks, _ = get_callbacks(output_dir=\"experiments\", model_name=f\"lstm{model_config}\")\n",
    "\n",
    "results = walk_forward_validation(\n",
    "    build_model_fn=lambda seq_len, num_features: build_lstm(seq_len, num_features, config=model_config),\n",
    "    seq_len=30, \n",
    "    n_splits=3, \n",
    "    batch_size=32, \n",
    "    epochs=20, \n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Convert list of dicts to DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find best split by lowest validation loss\n",
    "best_idx = results_df[\"val_loss\"].idxmin()\n",
    "best_result = results_df.iloc[best_idx]\n",
    "\n",
    "print(\" BEST LSTM MODEL SUMMARY\")\n",
    "print(\"\")\n",
    "print(f\"Best Split #: {best_result['split']}\")\n",
    "print(f\"Validation Loss : {best_result['val_loss']:.4f}\")\n",
    "print(f\"Validation Accuracy : {best_result['val_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy : {best_result['accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score : {best_result['f1']:.4f}\")\n",
    "print(f\"Test ROC-AUC : {best_result['roc_auc']:.4f}\")\n",
    "print(f\"Saved Model Path : {best_result['model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f66852",
   "metadata": {},
   "source": [
    "### GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f51890f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7643 - loss: 0.5702\n",
      "Epoch 1: val_loss improved from inf to 0.68238, saving model to experiments\\gru_medium_20250923-185219\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 80ms/step - accuracy: 0.7635 - loss: 0.5710 - val_accuracy: 0.5651 - val_loss: 0.6824 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6913 - loss: 0.5564\n",
      "Epoch 2: val_loss did not improve from 0.68238\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.6909 - loss: 0.5570 - val_accuracy: 0.4007 - val_loss: 0.7608 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7621 - loss: 0.5103\n",
      "Epoch 3: val_loss did not improve from 0.68238\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.7611 - loss: 0.5112 - val_accuracy: 0.3870 - val_loss: 0.7954 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7338 - loss: 0.4717\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68238\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - accuracy: 0.7327 - loss: 0.4727 - val_accuracy: 0.3870 - val_loss: 0.7956 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7226 - loss: 0.4547\n",
      "Epoch 5: val_loss did not improve from 0.68238\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - accuracy: 0.7209 - loss: 0.4567 - val_accuracy: 0.3870 - val_loss: 0.7999 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7195 - loss: 0.4454\n",
      "Epoch 6: val_loss did not improve from 0.68238\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.7184 - loss: 0.4465 - val_accuracy: 0.3870 - val_loss: 0.7846 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "\n",
      "Split 1 Results: {'split': 1, 'accuracy': 0.4883401920438957, 'precision': 0.4848920863309353, 'recall': 0.9573863636363636, 'f1': 0.6437440305635148, 'roc_auc': 0.4643567639257295, 'val_loss': 0.6823775172233582, 'val_accuracy': 0.5650684833526611, 'model_path': 'experiments\\\\gru_medium_20250923-185219\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7311 - loss: 0.5781\n",
      "Epoch 1: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.7307 - loss: 0.5789 - val_accuracy: 0.4475 - val_loss: 0.7717 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6894 - loss: 0.5551\n",
      "Epoch 2: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.6893 - loss: 0.5561 - val_accuracy: 0.4612 - val_loss: 0.7924 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6563 - loss: 0.5636\n",
      "Epoch 3: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 96ms/step - accuracy: 0.6562 - loss: 0.5642 - val_accuracy: 0.4612 - val_loss: 0.9025 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6892 - loss: 0.5478\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 93ms/step - accuracy: 0.6891 - loss: 0.5486 - val_accuracy: 0.4612 - val_loss: 1.1134 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6782 - loss: 0.5298\n",
      "Epoch 5: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53075s\u001b[0m 112ms/step - accuracy: 0.6777 - loss: 0.5305 - val_accuracy: 0.4612 - val_loss: 1.2796 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7481 - loss: 0.5023\n",
      "Epoch 6: val_loss did not improve from 0.68238\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 107ms/step - accuracy: 0.7475 - loss: 0.5030 - val_accuracy: 0.4612 - val_loss: 1.4972 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step\n",
      "\n",
      "Split 2 Results: {'split': 2, 'accuracy': 0.5459533607681756, 'precision': 0.4894179894179894, 'recall': 0.5727554179566563, 'f1': 0.5278174037089871, 'roc_auc': 0.5761106620506642, 'val_loss': 0.7716673612594604, 'val_accuracy': 0.44748857617378235, 'model_path': 'experiments\\\\gru_medium_20250923-185219\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4327 - loss: 0.6574\n",
      "Epoch 1: val_loss improved from 0.68238 to 0.66701, saving model to experiments\\gru_medium_20250923-185219\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.4394 - loss: 0.6555 - val_accuracy: 0.5646 - val_loss: 0.6670 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6776 - loss: 0.6063\n",
      "Epoch 2: val_loss did not improve from 0.66701\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.6776 - loss: 0.6059 - val_accuracy: 0.4762 - val_loss: 0.7000 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6508 - loss: 0.5924\n",
      "Epoch 3: val_loss did not improve from 0.66701\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.6521 - loss: 0.5917 - val_accuracy: 0.5306 - val_loss: 0.6782 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6725 - loss: 0.5898\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.66701\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.6729 - loss: 0.5893 - val_accuracy: 0.5102 - val_loss: 0.6895 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6590 - loss: 0.5588\n",
      "Epoch 5: val_loss did not improve from 0.66701\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.6589 - loss: 0.5592 - val_accuracy: 0.5102 - val_loss: 0.6853 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6872 - loss: 0.5734\n",
      "Epoch 6: val_loss did not improve from 0.66701\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6870 - loss: 0.5730 - val_accuracy: 0.5238 - val_loss: 0.6879 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "\n",
      "Split 3 Results: {'split': 3, 'accuracy': 0.6131687242798354, 'precision': 0.44485294117647056, 'recall': 0.4801587301587302, 'f1': 0.4618320610687023, 'roc_auc': 0.618199061595288, 'val_loss': 0.6670122742652893, 'val_accuracy': 0.5646258592605591, 'model_path': 'experiments\\\\gru_medium_20250923-185219\\\\best_model.h5'}\n",
      "\n",
      " BEST GRU MODEL SUMMARY\n",
      "Best Split #: 3\n",
      "Validation Loss : 0.6670\n",
      "Validation Accuracy : 0.5646\n",
      "Test Accuracy : 0.6132\n",
      "Test F1 Score : 0.4618\n",
      "Test ROC-AUC : 0.6182\n",
      "Saved Model Path : experiments\\gru_medium_20250923-185219\\best_model.h5\n"
     ]
    }
   ],
   "source": [
    "def build_gru(seq_len, num_features, config=\"medium\"):\n",
    "    \"\"\"\n",
    "    Build GRU model based on config size: 'small', 'medium', 'large'\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        gru_units = [64]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        gru_units = [64, 32]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"large\":\n",
    "        gru_units = [128, 64]\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.4\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # GRU layers\n",
    "    for i, units in enumerate(gru_units):\n",
    "        return_sequences = (i < len(gru_units) - 1)\n",
    "        model.add(tf.keras.layers.GRU(units, return_sequences=return_sequences))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_config = \"medium\"\n",
    "\n",
    "callbacks, _ = get_callbacks(output_dir=\"experiments\", model_name=f\"gru_{model_config}\")\n",
    "\n",
    "results_gru = walk_forward_validation(\n",
    "    build_model_fn=lambda seq_len, num_features: build_gru(seq_len, num_features, config=model_config),\n",
    "    seq_len=60, \n",
    "    n_splits=3, \n",
    "    batch_size=32, \n",
    "    epochs=20, \n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "results_gru_df = pd.DataFrame(results_gru)\n",
    "best_idx = results_gru_df[\"val_loss\"].idxmin()\n",
    "best_result = results_gru_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n BEST GRU MODEL SUMMARY\")\n",
    "print(f\"Best Split #: {best_result['split']}\")\n",
    "print(f\"Validation Loss : {best_result['val_loss']:.4f}\")\n",
    "print(f\"Validation Accuracy : {best_result['val_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy : {best_result['accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score : {best_result['f1']:.4f}\")\n",
    "print(f\"Test ROC-AUC : {best_result['roc_auc']:.4f}\")\n",
    "print(f\"Saved Model Path : {best_result['model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365de2e",
   "metadata": {},
   "source": [
    "### Conv1D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6a701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.5796703296703297, 1: 3.6379310344827585}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "seq_len = 20  # length of input sequence (days) \n",
    "test_size = 0.2\n",
    "n_splits = 3\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(\"Run make_labels.py first to generate labeled dataset\")\n",
    "\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "look_ahead = 3  # days\n",
    "threshold = 0.01  # +1% move\n",
    "\n",
    "df['future_return'] = df['close'].shift(-look_ahead) / df['close'] - 1\n",
    "df['target'] = (df['future_return'] > threshold).astype(int)\n",
    "\n",
    "# Build sequences\n",
    "# X, y = build_sequences(df, seq_len)\n",
    "X, y = build_sequences_filtered(df, seq_len)\n",
    "\n",
    "results = []\n",
    "total_len = len(X)\n",
    "split_len = int(total_len * test_size)  # test size per split\n",
    "\n",
    "for i in range(n_splits):\n",
    "    # Define rolling window indices\n",
    "    end_test = total_len - i * split_len\n",
    "    start_test = end_test - split_len\n",
    "    end_train = start_test - 1\n",
    "\n",
    "    X_train, y_train = X[:end_train], y[:end_train]\n",
    "    X_test, y_test = X[start_test:end_test], y[start_test:end_test]\n",
    "\n",
    "# Compute class weights\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00974999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m77/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6296 - loss: 0.5973\n",
      "Epoch 1: val_loss improved from inf to 0.68003, saving model to experiments\\conv1d_medium_20250923-184550\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.6294 - loss: 0.6034 - val_accuracy: 0.6182 - val_loss: 0.6800 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m81/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6334 - loss: 0.5774\n",
      "Epoch 2: val_loss did not improve from 0.68003\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6331 - loss: 0.5796 - val_accuracy: 0.5811 - val_loss: 0.6804 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m82/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6127 - loss: 0.5654\n",
      "Epoch 3: val_loss did not improve from 0.68003\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6127 - loss: 0.5669 - val_accuracy: 0.5642 - val_loss: 0.6825 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m79/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6154 - loss: 0.5544\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68003\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6156 - loss: 0.5590 - val_accuracy: 0.5135 - val_loss: 0.6930 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m80/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6404 - loss: 0.5439\n",
      "Epoch 5: val_loss did not improve from 0.68003\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6399 - loss: 0.5474 - val_accuracy: 0.5203 - val_loss: 0.6988 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m76/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6289 - loss: 0.5448\n",
      "Epoch 6: val_loss did not improve from 0.68003\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6292 - loss: 0.5512 - val_accuracy: 0.5000 - val_loss: 0.7058 - learning_rate: 5.0000e-05\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "Split 1 Results: {'split': 1, 'accuracy': 0.4843962008141113, 'precision': 0.4794069192751236, 'recall': 0.819718309859155, 'f1': 0.604989604989605, 'roc_auc': 0.4562790354693606, 'val_loss': 0.6800291538238525, 'val_accuracy': 0.6182432174682617, 'model_path': 'experiments\\\\conv1d_medium_20250923-184550\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m58/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5872 - loss: 0.5878\n",
      "Epoch 1: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5898 - loss: 0.5925 - val_accuracy: 0.4685 - val_loss: 0.6962 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6469 - loss: 0.5894\n",
      "Epoch 2: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6470 - loss: 0.5898 - val_accuracy: 0.4865 - val_loss: 0.6987 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m62/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6687 - loss: 0.5663\n",
      "Epoch 3: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6688 - loss: 0.5677 - val_accuracy: 0.5045 - val_loss: 0.7057 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m61/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6949 - loss: 0.5546\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6942 - loss: 0.5563 - val_accuracy: 0.5090 - val_loss: 0.7332 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m59/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6798 - loss: 0.5397\n",
      "Epoch 5: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6800 - loss: 0.5435 - val_accuracy: 0.5090 - val_loss: 0.7743 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m57/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7135 - loss: 0.5357\n",
      "Epoch 6: val_loss did not improve from 0.68003\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7114 - loss: 0.5414 - val_accuracy: 0.5090 - val_loss: 0.8294 - learning_rate: 5.0000e-05\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "Split 2 Results: {'split': 2, 'accuracy': 0.5332428765264586, 'precision': 0.46153846153846156, 'recall': 0.3507692307692308, 'f1': 0.3986013986013986, 'roc_auc': 0.5483271097834205, 'val_loss': 0.6962110996246338, 'val_accuracy': 0.46846845746040344, 'model_path': 'experiments\\\\conv1d_medium_20250923-184550\\\\best_model.h5'}\n",
      "Epoch 1/20\n",
      "\u001b[1m36/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1797 - loss: 0.6780\n",
      "Epoch 1: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.1830 - loss: 0.6756 - val_accuracy: 0.3041 - val_loss: 0.7140 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m37/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3088 - loss: 0.6474\n",
      "Epoch 2: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3113 - loss: 0.6468 - val_accuracy: 0.3108 - val_loss: 0.7173 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m35/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3816 - loss: 0.5935\n",
      "Epoch 3: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3853 - loss: 0.5985 - val_accuracy: 0.3649 - val_loss: 0.7133 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4678 - loss: 0.5973\n",
      "Epoch 4: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4680 - loss: 0.5976 - val_accuracy: 0.4189 - val_loss: 0.7086 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m38/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4986 - loss: 0.6051\n",
      "Epoch 5: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4980 - loss: 0.6036 - val_accuracy: 0.4459 - val_loss: 0.7024 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5372 - loss: 0.5702\n",
      "Epoch 6: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5378 - loss: 0.5710 - val_accuracy: 0.4932 - val_loss: 0.6951 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m37/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5699 - loss: 0.5538 \n",
      "Epoch 7: val_loss did not improve from 0.68003\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5699 - loss: 0.5551 - val_accuracy: 0.5068 - val_loss: 0.6860 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5546 - loss: 0.5787\n",
      "Epoch 8: val_loss improved from 0.68003 to 0.67993, saving model to experiments\\conv1d_medium_20250923-184550\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5548 - loss: 0.5778 - val_accuracy: 0.5203 - val_loss: 0.6799 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m36/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5678 - loss: 0.5361\n",
      "Epoch 9: val_loss improved from 0.67993 to 0.67026, saving model to experiments\\conv1d_medium_20250923-184550\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5733 - loss: 0.5378 - val_accuracy: 0.5270 - val_loss: 0.6703 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m36/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6097 - loss: 0.5198\n",
      "Epoch 10: val_loss did not improve from 0.67026\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6085 - loss: 0.5227 - val_accuracy: 0.5203 - val_loss: 0.6707 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6079 - loss: 0.5159\n",
      "Epoch 11: val_loss did not improve from 0.67026\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6080 - loss: 0.5195 - val_accuracy: 0.5203 - val_loss: 0.6729 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6023 - loss: 0.5373\n",
      "Epoch 12: val_loss improved from 0.67026 to 0.66993, saving model to experiments\\conv1d_medium_20250923-184550\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6036 - loss: 0.5365 - val_accuracy: 0.5270 - val_loss: 0.6699 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m36/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6389 - loss: 0.5515\n",
      "Epoch 13: val_loss did not improve from 0.66993\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6385 - loss: 0.5466 - val_accuracy: 0.5203 - val_loss: 0.6746 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m37/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6771 - loss: 0.5028\n",
      "Epoch 14: val_loss did not improve from 0.66993\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6765 - loss: 0.5028 - val_accuracy: 0.5135 - val_loss: 0.6726 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m37/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6644 - loss: 0.5009\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.66993\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6651 - loss: 0.5029 - val_accuracy: 0.5068 - val_loss: 0.6753 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6978 - loss: 0.5047\n",
      "Epoch 16: val_loss did not improve from 0.66993\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6973 - loss: 0.5053 - val_accuracy: 0.5270 - val_loss: 0.6779 - learning_rate: 5.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6815 - loss: 0.5184\n",
      "Epoch 17: val_loss did not improve from 0.66993\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6807 - loss: 0.5171 - val_accuracy: 0.5135 - val_loss: 0.6843 - learning_rate: 5.0000e-05\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\n",
      "Split 3 Results: {'split': 3, 'accuracy': 0.5345997286295794, 'precision': 0.4150197628458498, 'recall': 0.8171206225680934, 'f1': 0.5504587155963303, 'roc_auc': 0.6421530479896239, 'val_loss': 0.6699334979057312, 'val_accuracy': 0.5270270109176636, 'model_path': 'experiments\\\\conv1d_medium_20250923-184550\\\\best_model.h5'}\n",
      "\n",
      " BEST CONV1D MODEL SUMMARY\n",
      "Best Split #: 3\n",
      "Validation Loss : 0.6699\n",
      "Validation Accuracy : 0.5270\n",
      "Test Accuracy : 0.5346\n",
      "Test F1 Score : 0.5505\n",
      "Test ROC-AUC : 0.6422\n",
      "Saved Model Path : experiments\\conv1d_medium_20250923-184550\\best_model.h5\n"
     ]
    }
   ],
   "source": [
    "def build_conv1d(seq_len, num_features, config=\"medium\"):\n",
    "    \"\"\"\n",
    "    Improved Conv1D model for time series classification\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        filters = [32]\n",
    "        kernel_size = 3\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.2\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        filters = [64, 32]\n",
    "        kernel_size = 3\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.2\n",
    "\n",
    "    elif config == \"large\":\n",
    "        filters = [128, 64, 32]\n",
    "        kernel_size = 5\n",
    "        dense_units = [128, 64, 32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # === Conv1D Layers ===\n",
    "    for f in filters:\n",
    "        model.add(tf.keras.layers.Conv1D(filters=f, kernel_size=kernel_size, activation=\"relu\", padding=\"causal\"))\n",
    "        model.add(tf.keras.layers.BatchNormalization())  # <-- helps stabilize training\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Replace Flatten with Global Pooling\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # === Dense Layers ===\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # reduced LR\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_config = \"medium\"\n",
    "\n",
    "callbacks, _ = get_callbacks(output_dir=\"experiments\", model_name=f\"conv1d_{model_config}\")\n",
    "\n",
    "results_conv = walk_forward_validation(\n",
    "    build_model_fn=lambda seq_len, num_features: build_conv1d(seq_len, num_features, config=model_config),\n",
    "    seq_len=20, \n",
    "    n_splits=3, \n",
    "    batch_size=32, \n",
    "    epochs=20, \n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "results_conv_df = pd.DataFrame(results_conv)\n",
    "best_idx = results_conv_df[\"val_loss\"].idxmin()\n",
    "best_result = results_conv_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n BEST CONV1D MODEL SUMMARY\")\n",
    "print(f\"Best Split #: {best_result['split']}\")\n",
    "print(f\"Validation Loss : {best_result['val_loss']:.4f}\")\n",
    "print(f\"Validation Accuracy : {best_result['val_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy : {best_result['accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score : {best_result['f1']:.4f}\")\n",
    "print(f\"Test ROC-AUC : {best_result['roc_auc']:.4f}\")\n",
    "print(f\"Saved Model Path : {best_result['model_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
