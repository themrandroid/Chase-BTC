{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3daa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_save_models.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b37c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_DIR = \"data/features\"\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_labeled.parquet\")\n",
    "EXPERIMENTS_DIR = \"experiments\"\n",
    "SEQ_LEN = 20\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "N_SPLITS = 7  # for OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03ed1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(seq_len, num_features, config=\"medium\"):\n",
    "    \"\"\"\n",
    "    Build LSTM model based on config size: 'small', 'medium', 'large'\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        lstm_units = [64]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        lstm_units = [64, 32]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"large\":\n",
    "        lstm_units = [128, 64]\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.4\n",
    "\n",
    "    # Build Sequential Model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # Add LSTM layers\n",
    "    for i, units in enumerate(lstm_units):\n",
    "        return_sequences = (i < len(lstm_units) - 1)  # True except last LSTM\n",
    "        model.add(tf.keras.layers.LSTM(units, return_sequences=return_sequences))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Add Dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_gru(seq_len, num_features, config=\"medium\"):\n",
    "    \"\"\"\n",
    "    Build GRU model based on config size: 'small', 'medium', 'large'\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        gru_units = [64]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        gru_units = [64, 32]\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"large\":\n",
    "        gru_units = [128, 64]\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.4\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # GRU layers\n",
    "    for i, units in enumerate(gru_units):\n",
    "        return_sequences = (i < len(gru_units) - 1)\n",
    "        model.add(tf.keras.layers.GRU(units, return_sequences=return_sequences))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_conv1d(seq_len, num_features, config=\"medium\"):\n",
    "    \"\"\"\n",
    "    Build Conv1D model for time series\n",
    "    \"\"\"\n",
    "    if config == \"small\":\n",
    "        filters = [32]\n",
    "        kernel_size = 3\n",
    "        dense_units = [32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"medium\":\n",
    "        filters = [64, 32]\n",
    "        kernel_size = 3\n",
    "        dense_units = [64, 32]\n",
    "        dropout_rate = 0.3\n",
    "\n",
    "    elif config == \"large\":\n",
    "        filters = [128, 64]\n",
    "        kernel_size = 5\n",
    "        dense_units = [128, 64, 32]\n",
    "        dropout_rate = 0.4\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(seq_len, num_features)))\n",
    "\n",
    "    # Conv1D layers\n",
    "    for i, f in enumerate(filters):\n",
    "        model.add(tf.keras.layers.Conv1D(filters=f, kernel_size=kernel_size, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db48e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----1. Build Sequences -----------\n",
    "TOP_FEATURES = ['volatility_21d', 'volatility_10d', 'return_14d', 'return_3d', 'bollinger_down']\n",
    "\n",
    "def build_sequences(df, seq_len=20, target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Build sequences but only with top selected features\n",
    "    \"\"\"\n",
    "    values = df[TOP_FEATURES].values  # Use only top features\n",
    "    targets = df[target_col].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(targets[i+seq_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ----2. Get Callbacks -----------\n",
    "def get_callbacks(output_dir=\"experiments\", model_name=\"model\"):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(output_dir, f\"{model_name}_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(run_dir, \"best_model.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            filename=os.path.join(run_dir, \"training_log.csv\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return callbacks, run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c989d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: (3689, 20, 5) (3689,)\n",
      "\n",
      " Training lstm Fold 1/7\n",
      "Epoch 1/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7009 - loss: 0.5997\n",
      "Epoch 1: val_loss improved from inf to 0.39388, saving model to experiments\\lstm\\lstm_fold1_20250928-203108\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.7117 - loss: 0.5924 - val_accuracy: 0.8677 - val_loss: 0.3939 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.1278\n",
      "Epoch 2: val_loss did not improve from 0.39388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.1253 - val_accuracy: 0.8677 - val_loss: 0.5298 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0173\n",
      "Epoch 3: val_loss did not improve from 0.39388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 0.8677 - val_loss: 0.8279 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0067\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.39388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.8677 - val_loss: 0.9676 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 5: val_loss did not improve from 0.39388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.8677 - val_loss: 1.0035 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 6: val_loss did not improve from 0.39388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.8677 - val_loss: 1.0297 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 2/7\n",
      "Epoch 1/30\n",
      "\u001b[1m27/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6222 - loss: 0.6536\n",
      "Epoch 1: val_loss improved from inf to 0.60773, saving model to experiments\\lstm\\lstm_fold2_20250928-203125\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.6217 - loss: 0.6487 - val_accuracy: 0.6421 - val_loss: 0.6077 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6511 - loss: 0.4670\n",
      "Epoch 2: val_loss improved from 0.60773 to 0.59580, saving model to experiments\\lstm\\lstm_fold2_20250928-203125\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6518 - loss: 0.4677 - val_accuracy: 0.6768 - val_loss: 0.5958 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6957 - loss: 0.4718\n",
      "Epoch 3: val_loss improved from 0.59580 to 0.57269, saving model to experiments\\lstm\\lstm_fold2_20250928-203125\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6987 - loss: 0.4708 - val_accuracy: 0.7440 - val_loss: 0.5727 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m26/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8259 - loss: 0.3966\n",
      "Epoch 4: val_loss did not improve from 0.57269\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8283 - loss: 0.3982 - val_accuracy: 0.7636 - val_loss: 0.5805 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m26/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8273 - loss: 0.4176\n",
      "Epoch 5: val_loss did not improve from 0.57269\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8300 - loss: 0.4115 - val_accuracy: 0.7397 - val_loss: 0.6444 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8044 - loss: 0.3278\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.57269\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8035 - loss: 0.3286 - val_accuracy: 0.6377 - val_loss: 0.9023 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7564 - loss: 0.3355\n",
      "Epoch 7: val_loss did not improve from 0.57269\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7567 - loss: 0.3361 - val_accuracy: 0.7072 - val_loss: 0.7302 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8093 - loss: 0.3485\n",
      "Epoch 8: val_loss did not improve from 0.57269\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8088 - loss: 0.3462 - val_accuracy: 0.7093 - val_loss: 0.8108 - learning_rate: 5.0000e-04\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 3/7\n",
      "Epoch 1/30\n",
      "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4905 - loss: 0.6737\n",
      "Epoch 1: val_loss improved from inf to 0.67439, saving model to experiments\\lstm\\lstm_fold3_20250928-203137\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.4929 - loss: 0.6732 - val_accuracy: 0.6226 - val_loss: 0.6744 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6370 - loss: 0.6112\n",
      "Epoch 2: val_loss did not improve from 0.67439\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6376 - loss: 0.6115 - val_accuracy: 0.5597 - val_loss: 0.6937 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5943 - loss: 0.6073\n",
      "Epoch 3: val_loss did not improve from 0.67439\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5948 - loss: 0.6078 - val_accuracy: 0.5141 - val_loss: 0.7243 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5898 - loss: 0.5884\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.67439\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5923 - loss: 0.5893 - val_accuracy: 0.5358 - val_loss: 0.7511 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6810 - loss: 0.5477\n",
      "Epoch 5: val_loss did not improve from 0.67439\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6812 - loss: 0.5481 - val_accuracy: 0.5770 - val_loss: 0.7483 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7341 - loss: 0.5421\n",
      "Epoch 6: val_loss did not improve from 0.67439\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7337 - loss: 0.5419 - val_accuracy: 0.5835 - val_loss: 0.7228 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 4/7\n",
      "Epoch 1/30\n",
      "\u001b[1m57/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6779 - loss: 0.6650\n",
      "Epoch 1: val_loss improved from inf to 0.68805, saving model to experiments\\lstm\\lstm_fold4_20250928-203151\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - accuracy: 0.6767 - loss: 0.6655 - val_accuracy: 0.5423 - val_loss: 0.6880 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m56/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5933 - loss: 0.5839\n",
      "Epoch 2: val_loss did not improve from 0.68805\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5931 - loss: 0.5873 - val_accuracy: 0.5011 - val_loss: 0.7169 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5659 - loss: 0.6221\n",
      "Epoch 3: val_loss did not improve from 0.68805\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5662 - loss: 0.6225 - val_accuracy: 0.5380 - val_loss: 0.7481 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7015 - loss: 0.5526\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68805\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.7005 - loss: 0.5533 - val_accuracy: 0.4534 - val_loss: 0.8790 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m57/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7297 - loss: 0.4678\n",
      "Epoch 5: val_loss did not improve from 0.68805\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7267 - loss: 0.4705 - val_accuracy: 0.4252 - val_loss: 0.8870 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m54/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7190 - loss: 0.4721\n",
      "Epoch 6: val_loss did not improve from 0.68805\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7132 - loss: 0.4772 - val_accuracy: 0.4187 - val_loss: 0.9129 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 5/7\n",
      "Epoch 1/30\n",
      "\u001b[1m70/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7651 - loss: 0.6011\n",
      "Epoch 1: val_loss improved from inf to 0.74599, saving model to experiments\\lstm\\lstm_fold5_20250928-203207\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.7621 - loss: 0.6039 - val_accuracy: 0.4794 - val_loss: 0.7460 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m70/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7477 - loss: 0.5501\n",
      "Epoch 2: val_loss did not improve from 0.74599\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7445 - loss: 0.5541 - val_accuracy: 0.5054 - val_loss: 0.7849 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7565 - loss: 0.5507\n",
      "Epoch 3: val_loss did not improve from 0.74599\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7558 - loss: 0.5513 - val_accuracy: 0.5119 - val_loss: 0.8173 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7101 - loss: 0.5143\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.74599\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7094 - loss: 0.5153 - val_accuracy: 0.4837 - val_loss: 0.8185 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m71/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7216 - loss: 0.4950\n",
      "Epoch 5: val_loss did not improve from 0.74599\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7201 - loss: 0.4977 - val_accuracy: 0.4577 - val_loss: 0.8282 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7282 - loss: 0.4595\n",
      "Epoch 6: val_loss did not improve from 0.74599\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7271 - loss: 0.4605 - val_accuracy: 0.4577 - val_loss: 0.8323 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 6/7\n",
      "Epoch 1/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6745 - loss: 0.5977\n",
      "Epoch 1: val_loss improved from inf to 0.75128, saving model to experiments\\lstm\\lstm_fold6_20250928-203227\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.6733 - loss: 0.6004 - val_accuracy: 0.4534 - val_loss: 0.7513 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m86/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7274 - loss: 0.5495\n",
      "Epoch 2: val_loss did not improve from 0.75128\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7256 - loss: 0.5512 - val_accuracy: 0.4534 - val_loss: 0.8042 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m85/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7402 - loss: 0.5042\n",
      "Epoch 3: val_loss did not improve from 0.75128\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7370 - loss: 0.5070 - val_accuracy: 0.4534 - val_loss: 0.7955 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7362 - loss: 0.4664\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.75128\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.7351 - loss: 0.4674 - val_accuracy: 0.4534 - val_loss: 0.8414 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7422 - loss: 0.4522\n",
      "Epoch 5: val_loss did not improve from 0.75128\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7381 - loss: 0.4568 - val_accuracy: 0.4534 - val_loss: 0.8086 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m86/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7485 - loss: 0.4306\n",
      "Epoch 6: val_loss did not improve from 0.75128\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7460 - loss: 0.4330 - val_accuracy: 0.4534 - val_loss: 0.8118 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training lstm Fold 7/7\n",
      "Epoch 1/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6113 - loss: 0.5989\n",
      "Epoch 1: val_loss improved from inf to 0.82416, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.6107 - loss: 0.6006 - val_accuracy: 0.4729 - val_loss: 0.8242 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7573 - loss: 0.5304\n",
      "Epoch 2: val_loss improved from 0.82416 to 0.75705, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7546 - loss: 0.5328 - val_accuracy: 0.4729 - val_loss: 0.7570 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7487 - loss: 0.4804\n",
      "Epoch 3: val_loss did not improve from 0.75705\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7444 - loss: 0.4847 - val_accuracy: 0.4729 - val_loss: 0.7920 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6791 - loss: 0.4923\n",
      "Epoch 4: val_loss did not improve from 0.75705\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6784 - loss: 0.4932 - val_accuracy: 0.4729 - val_loss: 0.7918 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7427 - loss: 0.4584\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.75705\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7407 - loss: 0.4606 - val_accuracy: 0.4729 - val_loss: 0.7634 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7434 - loss: 0.4431\n",
      "Epoch 6: val_loss improved from 0.75705 to 0.75093, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7412 - loss: 0.4453 - val_accuracy: 0.4729 - val_loss: 0.7509 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7322 - loss: 0.4534\n",
      "Epoch 7: val_loss improved from 0.75093 to 0.74612, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7313 - loss: 0.4544 - val_accuracy: 0.4729 - val_loss: 0.7461 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7513 - loss: 0.4184\n",
      "Epoch 8: val_loss did not improve from 0.74612\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7480 - loss: 0.4223 - val_accuracy: 0.4729 - val_loss: 0.7499 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7355 - loss: 0.4300\n",
      "Epoch 9: val_loss improved from 0.74612 to 0.73827, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7324 - loss: 0.4336 - val_accuracy: 0.4729 - val_loss: 0.7383 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7521 - loss: 0.4104\n",
      "Epoch 10: val_loss improved from 0.73827 to 0.73116, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7478 - loss: 0.4157 - val_accuracy: 0.4729 - val_loss: 0.7312 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7400 - loss: 0.4277\n",
      "Epoch 11: val_loss did not improve from 0.73116\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7360 - loss: 0.4324 - val_accuracy: 0.4729 - val_loss: 0.7354 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7436 - loss: 0.4288\n",
      "Epoch 12: val_loss improved from 0.73116 to 0.73080, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7414 - loss: 0.4312 - val_accuracy: 0.4729 - val_loss: 0.7308 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7566 - loss: 0.4178\n",
      "Epoch 13: val_loss did not improve from 0.73080\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7544 - loss: 0.4203 - val_accuracy: 0.4729 - val_loss: 0.7333 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7509 - loss: 0.4187\n",
      "Epoch 14: val_loss improved from 0.73080 to 0.72364, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7464 - loss: 0.4235 - val_accuracy: 0.4729 - val_loss: 0.7236 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7461 - loss: 0.4035\n",
      "Epoch 15: val_loss did not improve from 0.72364\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7418 - loss: 0.4087 - val_accuracy: 0.4729 - val_loss: 0.7245 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7578 - loss: 0.4110\n",
      "Epoch 16: val_loss improved from 0.72364 to 0.72335, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7555 - loss: 0.4135 - val_accuracy: 0.4729 - val_loss: 0.7233 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7570 - loss: 0.4058\n",
      "Epoch 17: val_loss improved from 0.72335 to 0.72014, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7537 - loss: 0.4096 - val_accuracy: 0.4729 - val_loss: 0.7201 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7537 - loss: 0.4101\n",
      "Epoch 18: val_loss improved from 0.72014 to 0.71040, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7516 - loss: 0.4127 - val_accuracy: 0.4729 - val_loss: 0.7104 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7497 - loss: 0.4062\n",
      "Epoch 19: val_loss did not improve from 0.71040\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7487 - loss: 0.4075 - val_accuracy: 0.4729 - val_loss: 0.7155 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7558 - loss: 0.4029\n",
      "Epoch 20: val_loss did not improve from 0.71040\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7516 - loss: 0.4078 - val_accuracy: 0.4729 - val_loss: 0.7113 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7449 - loss: 0.4153\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.71040\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7418 - loss: 0.4188 - val_accuracy: 0.4729 - val_loss: 0.7177 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7693 - loss: 0.3940\n",
      "Epoch 22: val_loss did not improve from 0.71040\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7650 - loss: 0.3992 - val_accuracy: 0.4729 - val_loss: 0.7104 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7565 - loss: 0.3955\n",
      "Epoch 23: val_loss improved from 0.71040 to 0.70283, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7524 - loss: 0.4005 - val_accuracy: 0.4729 - val_loss: 0.7028 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7796 - loss: 0.3865\n",
      "Epoch 24: val_loss did not improve from 0.70283\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7784 - loss: 0.3878 - val_accuracy: 0.4729 - val_loss: 0.7055 - learning_rate: 2.5000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7532 - loss: 0.3907\n",
      "Epoch 25: val_loss improved from 0.70283 to 0.70241, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7491 - loss: 0.3957 - val_accuracy: 0.4881 - val_loss: 0.7024 - learning_rate: 2.5000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7622 - loss: 0.3881\n",
      "Epoch 26: val_loss improved from 0.70241 to 0.69730, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7603 - loss: 0.3906 - val_accuracy: 0.5033 - val_loss: 0.6973 - learning_rate: 2.5000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.3841\n",
      "Epoch 27: val_loss did not improve from 0.69730\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7648 - loss: 0.3895 - val_accuracy: 0.4902 - val_loss: 0.6978 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7671 - loss: 0.3805\n",
      "Epoch 28: val_loss improved from 0.69730 to 0.69441, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7650 - loss: 0.3832 - val_accuracy: 0.5033 - val_loss: 0.6944 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7730 - loss: 0.3904\n",
      "Epoch 29: val_loss improved from 0.69441 to 0.69017, saving model to experiments\\lstm\\lstm_fold7_20250928-203245\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7709 - loss: 0.3929 - val_accuracy: 0.5315 - val_loss: 0.6902 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7772 - loss: 0.3773\n",
      "Epoch 30: val_loss did not improve from 0.69017\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7740 - loss: 0.3812 - val_accuracy: 0.5445 - val_loss: 0.6914 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 1/7\n",
      "Epoch 1/30\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5865 - loss: 0.6524\n",
      "Epoch 1: val_loss improved from inf to 0.44353, saving model to experiments\\gru\\gru_fold1_20250928-203344\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 64ms/step - accuracy: 0.6003 - loss: 0.6456 - val_accuracy: 0.8677 - val_loss: 0.4435 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9945 - loss: 0.2316\n",
      "Epoch 2: val_loss improved from 0.44353 to 0.36733, saving model to experiments\\gru\\gru_fold1_20250928-203344\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9953 - loss: 0.2132 - val_accuracy: 0.8677 - val_loss: 0.3673 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0210\n",
      "Epoch 3: val_loss did not improve from 0.36733\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0196 - val_accuracy: 0.8677 - val_loss: 0.6111 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0048\n",
      "Epoch 4: val_loss did not improve from 0.36733\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.8677 - val_loss: 0.7757 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.36733\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.8677 - val_loss: 0.8510 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 6: val_loss did not improve from 0.36733\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.8677 - val_loss: 0.8725 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 7: val_loss did not improve from 0.36733\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.8677 - val_loss: 0.8887 - learning_rate: 5.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 2/7\n",
      "Epoch 1/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5592 - loss: 0.6457\n",
      "Epoch 1: val_loss improved from inf to 0.62319, saving model to experiments\\gru\\gru_fold2_20250928-203353\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.5637 - loss: 0.6425 - val_accuracy: 0.6356 - val_loss: 0.6232 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6904 - loss: 0.5207\n",
      "Epoch 2: val_loss improved from 0.62319 to 0.56670, saving model to experiments\\gru\\gru_fold2_20250928-203353\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6918 - loss: 0.5212 - val_accuracy: 0.6876 - val_loss: 0.5667 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m27/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7413 - loss: 0.5183\n",
      "Epoch 3: val_loss did not improve from 0.56670\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7386 - loss: 0.5187 - val_accuracy: 0.7093 - val_loss: 0.6021 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6608 - loss: 0.5253\n",
      "Epoch 4: val_loss did not improve from 0.56670\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6619 - loss: 0.5238 - val_accuracy: 0.6920 - val_loss: 0.5957 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m26/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7284 - loss: 0.4940\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.56670\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7278 - loss: 0.4932 - val_accuracy: 0.6941 - val_loss: 0.5886 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7351 - loss: 0.4130\n",
      "Epoch 6: val_loss did not improve from 0.56670\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7360 - loss: 0.4164 - val_accuracy: 0.6941 - val_loss: 0.6013 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m28/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7264 - loss: 0.4868\n",
      "Epoch 7: val_loss did not improve from 0.56670\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7263 - loss: 0.4850 - val_accuracy: 0.6963 - val_loss: 0.5971 - learning_rate: 5.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 3/7\n",
      "Epoch 1/30\n",
      "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7493 - loss: 0.6651\n",
      "Epoch 1: val_loss improved from inf to 0.67052, saving model to experiments\\gru\\gru_fold3_20250928-203403\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.7479 - loss: 0.6646 - val_accuracy: 0.6052 - val_loss: 0.6705 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6463 - loss: 0.6269\n",
      "Epoch 2: val_loss did not improve from 0.67052\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6460 - loss: 0.6273 - val_accuracy: 0.5271 - val_loss: 0.7010 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m42/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6068 - loss: 0.6351\n",
      "Epoch 3: val_loss did not improve from 0.67052\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6081 - loss: 0.6351 - val_accuracy: 0.5445 - val_loss: 0.6960 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6238 - loss: 0.6129\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.67052\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6242 - loss: 0.6130 - val_accuracy: 0.5445 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6483 - loss: 0.5865\n",
      "Epoch 5: val_loss did not improve from 0.67052\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6483 - loss: 0.5873 - val_accuracy: 0.5119 - val_loss: 0.6991 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6329 - loss: 0.6045\n",
      "Epoch 6: val_loss did not improve from 0.67052\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6329 - loss: 0.6045 - val_accuracy: 0.5271 - val_loss: 0.6992 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 4/7\n",
      "Epoch 1/30\n",
      "\u001b[1m56/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6435 - loss: 0.6194\n",
      "Epoch 1: val_loss improved from inf to 0.68552, saving model to experiments\\gru\\gru_fold4_20250928-203415\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6444 - loss: 0.6224 - val_accuracy: 0.5271 - val_loss: 0.6855 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m55/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5849 - loss: 0.6060\n",
      "Epoch 2: val_loss did not improve from 0.68552\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5851 - loss: 0.6094 - val_accuracy: 0.5076 - val_loss: 0.6929 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m57/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5773 - loss: 0.6108\n",
      "Epoch 3: val_loss did not improve from 0.68552\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5771 - loss: 0.6122 - val_accuracy: 0.5033 - val_loss: 0.7073 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m55/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6009 - loss: 0.5809\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68552\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5988 - loss: 0.5859 - val_accuracy: 0.4924 - val_loss: 0.7332 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m56/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5667 - loss: 0.5922\n",
      "Epoch 5: val_loss did not improve from 0.68552\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5674 - loss: 0.5944 - val_accuracy: 0.4946 - val_loss: 0.7558 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m57/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5634 - loss: 0.5743\n",
      "Epoch 6: val_loss did not improve from 0.68552\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5635 - loss: 0.5763 - val_accuracy: 0.5011 - val_loss: 0.7823 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 5/7\n",
      "Epoch 1/30\n",
      "\u001b[1m72/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6751 - loss: 0.6140\n",
      "Epoch 1: val_loss improved from inf to 0.71240, saving model to experiments\\gru\\gru_fold5_20250928-203428\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.6748 - loss: 0.6152 - val_accuracy: 0.5033 - val_loss: 0.7124 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m72/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7041 - loss: 0.5970\n",
      "Epoch 2: val_loss did not improve from 0.71240\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7034 - loss: 0.5979 - val_accuracy: 0.4989 - val_loss: 0.7460 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m72/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7587 - loss: 0.5229\n",
      "Epoch 3: val_loss did not improve from 0.71240\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7564 - loss: 0.5252 - val_accuracy: 0.5098 - val_loss: 0.7496 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6989 - loss: 0.5467\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71240\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6985 - loss: 0.5475 - val_accuracy: 0.4685 - val_loss: 0.7628 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7584 - loss: 0.5191\n",
      "Epoch 5: val_loss did not improve from 0.71240\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7575 - loss: 0.5201 - val_accuracy: 0.4729 - val_loss: 0.7877 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m70/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7435 - loss: 0.4959\n",
      "Epoch 6: val_loss did not improve from 0.71240\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7401 - loss: 0.5001 - val_accuracy: 0.4577 - val_loss: 0.8146 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 6/7\n",
      "Epoch 1/30\n",
      "\u001b[1m86/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7106 - loss: 0.5950\n",
      "Epoch 1: val_loss improved from inf to 0.76651, saving model to experiments\\gru\\gru_fold6_20250928-203443\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7095 - loss: 0.5964 - val_accuracy: 0.4946 - val_loss: 0.7665 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7192 - loss: 0.5486\n",
      "Epoch 2: val_loss did not improve from 0.76651\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7166 - loss: 0.5521 - val_accuracy: 0.4534 - val_loss: 0.8133 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7611 - loss: 0.5334\n",
      "Epoch 3: val_loss did not improve from 0.76651\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7603 - loss: 0.5342 - val_accuracy: 0.4534 - val_loss: 0.8589 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7370 - loss: 0.4863\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.76651\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7359 - loss: 0.4874 - val_accuracy: 0.4534 - val_loss: 0.8243 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m86/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7532 - loss: 0.4706\n",
      "Epoch 5: val_loss did not improve from 0.76651\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7510 - loss: 0.4727 - val_accuracy: 0.4534 - val_loss: 0.7875 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7530 - loss: 0.4223\n",
      "Epoch 6: val_loss did not improve from 0.76651\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7482 - loss: 0.4277 - val_accuracy: 0.4534 - val_loss: 0.7959 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training gru Fold 7/7\n",
      "Epoch 1/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7145 - loss: 0.5893\n",
      "Epoch 1: val_loss improved from inf to 0.85909, saving model to experiments\\gru\\gru_fold7_20250928-203459\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.7139 - loss: 0.5899 - val_accuracy: 0.4729 - val_loss: 0.8591 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7157 - loss: 0.5380\n",
      "Epoch 2: val_loss improved from 0.85909 to 0.85082, saving model to experiments\\gru\\gru_fold7_20250928-203459\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7149 - loss: 0.5388 - val_accuracy: 0.4729 - val_loss: 0.8508 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7368 - loss: 0.5199\n",
      "Epoch 3: val_loss improved from 0.85082 to 0.77867, saving model to experiments\\gru\\gru_fold7_20250928-203459\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7349 - loss: 0.5216 - val_accuracy: 0.4729 - val_loss: 0.7787 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7135 - loss: 0.4969\n",
      "Epoch 4: val_loss did not improve from 0.77867\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7119 - loss: 0.4988 - val_accuracy: 0.4729 - val_loss: 0.7891 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7465 - loss: 0.4620\n",
      "Epoch 5: val_loss did not improve from 0.77867\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7430 - loss: 0.4653 - val_accuracy: 0.4729 - val_loss: 0.8290 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7356 - loss: 0.4369\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.77867\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7333 - loss: 0.4395 - val_accuracy: 0.4729 - val_loss: 0.8267 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7428 - loss: 0.4435\n",
      "Epoch 7: val_loss improved from 0.77867 to 0.74884, saving model to experiments\\gru\\gru_fold7_20250928-203459\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.7417 - loss: 0.4446 - val_accuracy: 0.4729 - val_loss: 0.7488 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7357 - loss: 0.4311\n",
      "Epoch 8: val_loss did not improve from 0.74884\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7325 - loss: 0.4346 - val_accuracy: 0.4729 - val_loss: 0.7602 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m 99/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7226 - loss: 0.4409\n",
      "Epoch 9: val_loss did not improve from 0.74884\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7194 - loss: 0.4444 - val_accuracy: 0.4729 - val_loss: 0.7990 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7286 - loss: 0.4360\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.74884\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7264 - loss: 0.4384 - val_accuracy: 0.4729 - val_loss: 0.7616 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7434 - loss: 0.4259\n",
      "Epoch 11: val_loss did not improve from 0.74884\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7423 - loss: 0.4271 - val_accuracy: 0.4729 - val_loss: 0.7621 - learning_rate: 2.5000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7476 - loss: 0.4099\n",
      "Epoch 12: val_loss did not improve from 0.74884\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7464 - loss: 0.4112 - val_accuracy: 0.4729 - val_loss: 0.7632 - learning_rate: 2.5000e-04\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 1/7\n",
      "Epoch 1/30\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8731 - loss: 0.4464\n",
      "Epoch 1: val_loss improved from inf to 1.79770, saving model to experiments\\conv1d\\conv1d_fold1_20250928-203529\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.8942 - loss: 0.3980 - val_accuracy: 0.8677 - val_loss: 1.7977 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m 5/15\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 2: val_loss did not improve from 1.79770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.8677 - val_loss: 4.1469 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.5869e-04 \n",
      "Epoch 3: val_loss did not improve from 1.79770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.5308e-04 - val_accuracy: 0.8677 - val_loss: 5.0159 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.9739e-06 \n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.79770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2099e-05 - val_accuracy: 0.8677 - val_loss: 5.2672 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.4197e-05 \n",
      "Epoch 5: val_loss did not improve from 1.79770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.2002e-05 - val_accuracy: 0.8677 - val_loss: 5.3068 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.3756e-04 \n",
      "Epoch 6: val_loss did not improve from 1.79770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.3813e-04 - val_accuracy: 0.8677 - val_loss: 5.3509 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 2/7\n",
      "Epoch 1/30\n",
      "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6633 - loss: 0.7596\n",
      "Epoch 1: val_loss improved from inf to 0.61068, saving model to experiments\\conv1d\\conv1d_fold2_20250928-203533\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6749 - loss: 0.7319 - val_accuracy: 0.6768 - val_loss: 0.6107 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7235 - loss: 0.6250\n",
      "Epoch 2: val_loss did not improve from 0.61068\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7124 - loss: 0.5940 - val_accuracy: 0.7007 - val_loss: 0.6450 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7311 - loss: 0.4492\n",
      "Epoch 3: val_loss did not improve from 0.61068\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 0.4509 - val_accuracy: 0.6811 - val_loss: 0.7297 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7392 - loss: 0.4430\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.61068\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7345 - loss: 0.4472 - val_accuracy: 0.7375 - val_loss: 0.6939 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7892 - loss: 0.3744\n",
      "Epoch 5: val_loss did not improve from 0.61068\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7825 - loss: 0.3858 - val_accuracy: 0.6898 - val_loss: 0.7666 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m27/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7523 - loss: 0.4622\n",
      "Epoch 6: val_loss did not improve from 0.61068\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7550 - loss: 0.4560 - val_accuracy: 0.7527 - val_loss: 0.7428 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 3/7\n",
      "Epoch 1/30\n",
      "\u001b[1m41/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7306 - loss: 0.6980\n",
      "Epoch 1: val_loss improved from inf to 0.69741, saving model to experiments\\conv1d\\conv1d_fold3_20250928-203538\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7275 - loss: 0.6973 - val_accuracy: 0.5553 - val_loss: 0.6974 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6146 - loss: 0.6069\n",
      "Epoch 2: val_loss did not improve from 0.69741\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6139 - loss: 0.6079 - val_accuracy: 0.4577 - val_loss: 0.7375 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m33/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5815 - loss: 0.6197\n",
      "Epoch 3: val_loss did not improve from 0.69741\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5922 - loss: 0.6225 - val_accuracy: 0.5618 - val_loss: 0.7159 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6502 - loss: 0.5908\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69741\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6494 - loss: 0.5915 - val_accuracy: 0.5141 - val_loss: 0.7047 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6790 - loss: 0.5876\n",
      "Epoch 5: val_loss did not improve from 0.69741\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6790 - loss: 0.5873 - val_accuracy: 0.5488 - val_loss: 0.7170 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m43/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6921 - loss: 0.5372\n",
      "Epoch 6: val_loss did not improve from 0.69741\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6916 - loss: 0.5390 - val_accuracy: 0.5033 - val_loss: 0.7565 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 4/7\n",
      "Epoch 1/30\n",
      "\u001b[1m51/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6828 - loss: 0.6644\n",
      "Epoch 1: val_loss improved from inf to 0.69080, saving model to experiments\\conv1d\\conv1d_fold4_20250928-203543\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6732 - loss: 0.6700 - val_accuracy: 0.5466 - val_loss: 0.6908 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m52/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6215 - loss: 0.5996\n",
      "Epoch 2: val_loss did not improve from 0.69080\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6192 - loss: 0.6069 - val_accuracy: 0.5119 - val_loss: 0.7037 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m52/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6175 - loss: 0.5873\n",
      "Epoch 3: val_loss did not improve from 0.69080\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6150 - loss: 0.5948 - val_accuracy: 0.5054 - val_loss: 0.7269 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m49/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5784 - loss: 0.5888\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69080\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5764 - loss: 0.5971 - val_accuracy: 0.5163 - val_loss: 0.7340 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m53/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6416 - loss: 0.5575\n",
      "Epoch 5: val_loss did not improve from 0.69080\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6382 - loss: 0.5638 - val_accuracy: 0.4967 - val_loss: 0.7572 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m54/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6310 - loss: 0.5633\n",
      "Epoch 6: val_loss did not improve from 0.69080\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6312 - loss: 0.5677 - val_accuracy: 0.4946 - val_loss: 0.8000 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 5/7\n",
      "Epoch 1/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7403 - loss: 0.6112\n",
      "Epoch 1: val_loss improved from inf to 0.72217, saving model to experiments\\conv1d\\conv1d_fold5_20250928-203548\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7392 - loss: 0.6121 - val_accuracy: 0.5033 - val_loss: 0.7222 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m63/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6187 - loss: 0.5884\n",
      "Epoch 2: val_loss did not improve from 0.72217\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6201 - loss: 0.5948 - val_accuracy: 0.5163 - val_loss: 0.7472 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6972 - loss: 0.5551\n",
      "Epoch 3: val_loss did not improve from 0.72217\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6967 - loss: 0.5560 - val_accuracy: 0.5054 - val_loss: 0.7505 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m70/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7849 - loss: 0.5051\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.72217\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7808 - loss: 0.5098 - val_accuracy: 0.4534 - val_loss: 0.7846 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m64/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7371 - loss: 0.4951\n",
      "Epoch 5: val_loss did not improve from 0.72217\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7293 - loss: 0.5067 - val_accuracy: 0.4577 - val_loss: 0.7638 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m66/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7233 - loss: 0.4955\n",
      "Epoch 6: val_loss did not improve from 0.72217\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7170 - loss: 0.5046 - val_accuracy: 0.4577 - val_loss: 0.7658 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 6/7\n",
      "Epoch 1/30\n",
      "\u001b[1m86/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7602 - loss: 0.5677\n",
      "Epoch 1: val_loss improved from inf to 0.73867, saving model to experiments\\conv1d\\conv1d_fold6_20250928-203553\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7575 - loss: 0.5695 - val_accuracy: 0.4555 - val_loss: 0.7387 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7279 - loss: 0.5451\n",
      "Epoch 2: val_loss did not improve from 0.73867\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7251 - loss: 0.5488 - val_accuracy: 0.4534 - val_loss: 0.8710 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m84/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7471 - loss: 0.5112\n",
      "Epoch 3: val_loss did not improve from 0.73867\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7432 - loss: 0.5153 - val_accuracy: 0.4534 - val_loss: 0.7776 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m82/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7664 - loss: 0.4836\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.73867\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7604 - loss: 0.4903 - val_accuracy: 0.4534 - val_loss: 0.8115 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m76/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7184 - loss: 0.4909\n",
      "Epoch 5: val_loss did not improve from 0.73867\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7095 - loss: 0.5024 - val_accuracy: 0.4534 - val_loss: 0.7994 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m77/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7486 - loss: 0.4396\n",
      "Epoch 6: val_loss did not improve from 0.73867\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7360 - loss: 0.4545 - val_accuracy: 0.4534 - val_loss: 0.7804 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training conv1d Fold 7/7\n",
      "Epoch 1/30\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7399 - loss: 0.5795\n",
      "Epoch 1: val_loss improved from inf to 0.80101, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7390 - loss: 0.5803 - val_accuracy: 0.4729 - val_loss: 0.8010 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m 94/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7003 - loss: 0.5434\n",
      "Epoch 2: val_loss did not improve from 0.80101\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6953 - loss: 0.5501 - val_accuracy: 0.4729 - val_loss: 0.8336 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m 93/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7560 - loss: 0.4950\n",
      "Epoch 3: val_loss improved from 0.80101 to 0.79514, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7467 - loss: 0.5039 - val_accuracy: 0.4729 - val_loss: 0.7951 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m100/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7387 - loss: 0.4893\n",
      "Epoch 4: val_loss improved from 0.79514 to 0.76243, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7369 - loss: 0.4913 - val_accuracy: 0.4729 - val_loss: 0.7624 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m 97/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7182 - loss: 0.4915\n",
      "Epoch 5: val_loss improved from 0.76243 to 0.75623, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7141 - loss: 0.4959 - val_accuracy: 0.4729 - val_loss: 0.7562 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m 91/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7446 - loss: 0.4640\n",
      "Epoch 6: val_loss improved from 0.75623 to 0.72989, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7333 - loss: 0.4756 - val_accuracy: 0.4729 - val_loss: 0.7299 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m 94/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7791 - loss: 0.4232\n",
      "Epoch 7: val_loss improved from 0.72989 to 0.72485, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7694 - loss: 0.4333 - val_accuracy: 0.4729 - val_loss: 0.7249 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m 98/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7562 - loss: 0.4355\n",
      "Epoch 8: val_loss did not improve from 0.72485\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7519 - loss: 0.4402 - val_accuracy: 0.4729 - val_loss: 0.7421 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m 90/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7520 - loss: 0.4269\n",
      "Epoch 9: val_loss improved from 0.72485 to 0.71171, saving model to experiments\\conv1d\\conv1d_fold7_20250928-203559\\best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7402 - loss: 0.4409 - val_accuracy: 0.4729 - val_loss: 0.7117 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m 95/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7481 - loss: 0.4313\n",
      "Epoch 10: val_loss did not improve from 0.71171\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7410 - loss: 0.4393 - val_accuracy: 0.4729 - val_loss: 0.7358 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m 97/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 0.4238\n",
      "Epoch 11: val_loss did not improve from 0.71171\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7492 - loss: 0.4295 - val_accuracy: 0.4729 - val_loss: 0.7484 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m 95/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7646 - loss: 0.4237\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.71171\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7576 - loss: 0.4315 - val_accuracy: 0.4729 - val_loss: 0.7332 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m 94/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7727 - loss: 0.4181\n",
      "Epoch 13: val_loss did not improve from 0.71171\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7650 - loss: 0.4263 - val_accuracy: 0.4751 - val_loss: 0.7241 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m 97/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7742 - loss: 0.4005\n",
      "Epoch 14: val_loss did not improve from 0.71171\n",
      "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7692 - loss: 0.4065 - val_accuracy: 0.4707 - val_loss: 0.7131 - learning_rate: 5.0000e-04\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking dataset saved to experiments\\ensemble_20250928-203107\\stacking_oof.csv\n",
      "\n",
      "Averaging Ensemble - AUC: 0.7182, Acc: 0.6414, F1: 0.5664\n",
      "Meta fold 1: train 923, val 922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:36:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta fold 2: train 1845, val 922\n",
      "Meta fold 3: train 2767, val 922\n",
      "\n",
      "Optimal threshold for F1: 0.43 (F1=0.5779)\n",
      "\n",
      "Stacking Ensemble - AUC: 0.6464, Acc: 0.6099, F1: 0.3879\n",
      "Meta-learner saved to experiments\\ensemble_20250928-203107\\meta_model.pkl\n",
      "\n",
      "Comparison of Models (using OOF predictions):\n",
      "Note: Stacking Ensemble uses optimized threshold = 0.43\n",
      "           prob_lstm  prob_gru  prob_conv\n",
      "prob_lstm   1.000000  0.925605   0.905651\n",
      "prob_gru    0.925605  1.000000   0.954120\n",
      "prob_conv   0.905651  0.954120   1.000000\n",
      "LSTM                  AUC: 0.7158  Acc: 0.6552  F1: 0.5411\n",
      "GRU                   AUC: 0.7155  Acc: 0.6449  F1: 0.5429\n",
      "Conv1D                AUC: 0.7114  Acc: 0.6349  F1: 0.5787\n",
      "Averaging Ensemble    AUC: 0.7182  Acc: 0.6246  F1: 0.5779\n",
      "Stacking Ensemble     AUC: 0.6464  Acc: 0.6256  F1: 0.2743\n"
     ]
    }
   ],
   "source": [
    "# Generate OOF predictions\n",
    "def oof_predictions(X, y, build_fn, model_name):\n",
    "    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "    oof = np.zeros(len(X))\n",
    "    model_dir = os.path.join(EXPERIMENTS_DIR, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        fold += 1\n",
    "        print(f\"\\n Training {model_name} Fold {fold}/{N_SPLITS}\")\n",
    "\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # ---- 1. Compute class weights for this fold ----\n",
    "        classes = np.unique(y_tr)  # Use only training labels for this fold\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_tr)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "        \n",
    "        model = build_fn(SEQ_LEN, X.shape[2])\n",
    "        callbacks, run_dir = get_callbacks(output_dir=model_dir, model_name=f\"{model_name}_fold{fold}\")\n",
    "        \n",
    "        model.fit(\n",
    "            tf.data.Dataset.from_tensor_slices((X_tr, y_tr)).shuffle(1000).batch(BATCH_SIZE),\n",
    "            validation_data=tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        # load best weights if saved\n",
    "        best_path = os.path.join(run_dir, \"best_model.h5\")\n",
    "        if os.path.exists(best_path):\n",
    "            model.load_weights(best_path)\n",
    "\n",
    "        preds = model.predict(X_val).ravel()\n",
    "        oof[val_idx] = preds\n",
    "\n",
    "        # save fold model\n",
    "        model.save(os.path.join(model_dir, f\"{model_name}_fold{fold}.h5\"))\n",
    "\n",
    "    np.save(os.path.join(model_dir, f\"{model_name}_oof.npy\"), oof)\n",
    "    return oof\n",
    "\n",
    "def find_best_threshold(y_true, y_prob):\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)  # step size = 0.01\n",
    "    best_thr, best_f1 = 0.5, 0\n",
    "    for thr in thresholds:\n",
    "        f1 = f1_score(y_true, (y_prob > thr).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    return best_thr, best_f1\n",
    "\n",
    "def ensemble():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_dir = os.path.join(EXPERIMENTS_DIR, f\"ensemble_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(\"Run make_labels.py first to generate labeled dataset\")\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "    look_ahead = 3  # days\n",
    "    threshold = 0.01  # +1% move\n",
    "\n",
    "    df['future_return'] = df['close'].shift(-look_ahead) / df['close'] - 1\n",
    "    df['target'] = (df['future_return'] > threshold).astype(int)\n",
    "    X, y = build_sequences(df, seq_len=SEQ_LEN)\n",
    "    print(\"Data ready:\", X.shape, y.shape)\n",
    "\n",
    "    # Base model OOF predictions\n",
    "    oof_lstm = oof_predictions(X, y, build_lstm, \"lstm\")\n",
    "    oof_gru = oof_predictions(X, y, build_gru, \"gru\")\n",
    "    oof_conv = oof_predictions(X, y, build_conv1d, \"conv1d\")\n",
    "\n",
    "    # Save stacking dataset\n",
    "    stack_df = pd.DataFrame({\n",
    "        \"prob_lstm\": oof_lstm,\n",
    "        \"prob_gru\": oof_gru,\n",
    "        \"prob_conv\": oof_conv,\n",
    "        \"target\": y\n",
    "    })\n",
    "    stack_file = os.path.join(run_dir, \"stacking_oof.csv\")\n",
    "    stack_df.to_csv(stack_file, index=False)\n",
    "    print(f\"Stacking dataset saved to {stack_file}\")\n",
    "\n",
    "    # Averaging ensemble evaluation\n",
    "    avg_prob = stack_df[[\"prob_lstm\", \"prob_gru\", \"prob_conv\"]].mean(axis=1).values\n",
    "    avg_auc = roc_auc_score(y, avg_prob)\n",
    "    avg_acc = accuracy_score(y, (avg_prob > 0.5).astype(int))\n",
    "    avg_f1 = f1_score(y, (avg_prob > 0.5).astype(int))\n",
    "    print(f\"\\nAveraging Ensemble - AUC: {avg_auc:.4f}, Acc: {avg_acc:.4f}, F1: {avg_f1:.4f}\")\n",
    "\n",
    "    # Stacking ensemble (meta-learner)\n",
    "    X_stack = stack_df[[\"prob_lstm\", \"prob_gru\", \"prob_conv\"]].values\n",
    "    y_stack = stack_df[\"target\"].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_stack = scaler.fit_transform(X_stack)\n",
    "   \n",
    "    tscv_meta = TimeSeriesSplit(n_splits=3)\n",
    "    meta_oof = np.zeros(len(X_stack))  # store all meta predictions\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv_meta.split(X_stack)):\n",
    "        print(f\"Meta fold {fold+1}: train {len(train_idx)}, val {len(val_idx)}\")\n",
    "        X_train, X_hold = X_stack[train_idx], X_stack[val_idx]\n",
    "        y_train, y_hold = y_stack[train_idx], y_stack[val_idx]\n",
    "\n",
    "        meta = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=3,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "        meta.fit(X_train, y_train)\n",
    "        meta_oof[val_idx] = meta.predict_proba(X_hold)[:, 1]\n",
    "\n",
    "    # # --- Optimize Threshold ---\n",
    "    best_thr, best_f1 = find_best_threshold(y_stack, meta_oof)\n",
    "    best_thr = 0.5\n",
    "    best_thr, best_f1 = find_best_threshold(y, avg_prob)\n",
    "    print(f\"\\nOptimal threshold for F1: {best_thr:.2f} (F1={best_f1:.4f})\")\n",
    "\n",
    "    stack_auc = roc_auc_score(y_stack, meta_oof)\n",
    "    stack_acc = accuracy_score(y_stack, (meta_oof > best_thr).astype(int))\n",
    "    stack_f1 = f1_score(y_stack, (meta_oof > best_thr).astype(int))\n",
    "    print(f\"\\nStacking Ensemble - AUC: {stack_auc:.4f}, Acc: {stack_acc:.4f}, F1: {stack_f1:.4f}\")\n",
    "\n",
    "    # Save meta-learner\n",
    "    meta_file = os.path.join(run_dir, \"meta_model.pkl\")\n",
    "    joblib.dump(meta, meta_file)\n",
    "    print(f\"Meta-learner saved to {meta_file}\")\n",
    "\n",
    "    # # Compare base vs ensembles\n",
    "    print(\"\\nComparison of Models (using OOF predictions):\")\n",
    "    print(f\"Note: Stacking Ensemble uses optimized threshold = {best_thr:.2f}\")\n",
    "\n",
    "    print(stack_df[[\"prob_lstm\", \"prob_gru\", \"prob_conv\"]].corr())\n",
    "\n",
    "    for name, preds in {\n",
    "        \"LSTM\": oof_lstm,\n",
    "        \"GRU\": oof_gru,\n",
    "        \"Conv1D\": oof_conv,\n",
    "        \"Averaging Ensemble\": avg_prob,\n",
    "        \"Stacking Ensemble\": meta_oof  # full OOF for meta\n",
    "    }.items():\n",
    "        y_true = y  # use full target for all models\n",
    "        threshold = best_thr if name == \"Averaging Ensemble\" else 0.5\n",
    "        y_pred_binary = (preds > threshold).astype(int)\n",
    "\n",
    "        auc = roc_auc_score(y_true, preds)\n",
    "        acc = accuracy_score(y_true, y_pred_binary)\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "        print(f\"{name:20s}  AUC: {auc:.4f}  Acc: {acc:.4f}  F1: {f1:.4f}\")\n",
    "\n",
    "ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698839cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONFIG\n",
    "# EXPERIMENTS_DIR = \"experiments\"\n",
    "# ensemble_dirs = glob.glob(os.path.join(EXPERIMENTS_DIR, \"ensemble_*\"))\n",
    "# latest_ensemble_dir = max(ensemble_dirs, key=os.path.getmtime)\n",
    "# STACK_PATH = os.path.join(latest_ensemble_dir, \"stacking_oof.csv\")\n",
    "# print(\"Using stacking file:\", STACK_PATH)\n",
    "# OUTPUT_DIR = os.path.join(EXPERIMENTS_DIR, \"calibration\")\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# PLATT_FILE = os.path.join(OUTPUT_DIR, \"calibrator_platt.pkl\")\n",
    "# ISO_FILE = os.path.join(OUTPUT_DIR, \"calibrator_isotonic.pkl\")\n",
    "# REPORT_FILE = os.path.join(OUTPUT_DIR, \"calibration_report.json\")\n",
    "\n",
    "# def load_stack(stack_path=STACK_PATH):\n",
    "#     if not os.path.exists(stack_path):\n",
    "#         raise FileNotFoundError(f\"Stacking file not found at {stack_path}\")\n",
    "#     df = pd.read_csv(stack_path)\n",
    "#     # Expect columns: prob_lstm, prob_gru, prob_conv, target OR 'avg_prob' if present\n",
    "#     if \"avg_prob\" in df.columns:\n",
    "#         avg = df[\"avg_prob\"].values\n",
    "#     else:\n",
    "#         avg = df[[\"prob_lstm\", \"prob_gru\", \"prob_conv\"]].mean(axis=1).values\n",
    "#     y = df[\"target\"].values\n",
    "#     return avg, y, df\n",
    "\n",
    "# def train_calibrators(probs, y, holdout_frac=0.2, random_state=42):\n",
    "#     \"\"\"\n",
    "#     Train Platt (logistic) and Isotonic calibrators on OOF probs.\n",
    "#     Use time-aware split if your data is temporal: here we perform a simple holdout\n",
    "#     that preserves order by splitting by index (no shuffle).\n",
    "#     \"\"\"\n",
    "#     n = len(probs)\n",
    "#     split_idx = int(n * (1 - holdout_frac))\n",
    "\n",
    "#     # Time-preserving split (do not shuffle) - use first portion for train, last for holdout\n",
    "#     p_train, p_hold = probs[:split_idx], probs[split_idx:]\n",
    "#     y_train, y_hold = y[:split_idx], y[split_idx:]\n",
    "\n",
    "#     # --- Platt (LogisticRegression on single-column probs) ---\n",
    "#     platt = LogisticRegression(max_iter=1000)\n",
    "#     platt.fit(p_train.reshape(-1, 1), y_train)\n",
    "\n",
    "#     # --- Isotonic ---\n",
    "#     iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "#     iso.fit(p_train, y_train)\n",
    "\n",
    "#     # Evaluate on holdout\n",
    "#     results = {}\n",
    "#     for name, calib in [(\"platt\", platt), (\"isotonic\", iso)]:\n",
    "#         if name == \"platt\":\n",
    "#             p_cal = platt.predict_proba(p_hold.reshape(-1, 1))[:, 1]\n",
    "#         else:\n",
    "#             p_cal = iso.predict(p_hold)\n",
    "\n",
    "#         p_cal = np.clip(p_cal, 1e-15, 1 - 1e-15)  # avoid log(0)\n",
    "#         results[name] = {\n",
    "#             \"brier\": float(brier_score_loss(y_hold, p_cal)),\n",
    "#             \"logloss\": float(log_loss(y_hold, p_cal)),\n",
    "#             \"roc_auc\": float(roc_auc_score(y_hold, p_cal))\n",
    "#         }\n",
    "\n",
    "#     # Save calibrators and holdout split indices for traceability\n",
    "#     joblib.dump(platt, PLATT_FILE)\n",
    "#     joblib.dump(iso, ISO_FILE)\n",
    "\n",
    "#     report = {\n",
    "#         \"n_samples\": int(n),\n",
    "#         \"holdout_start_index\": int(split_idx),\n",
    "#         \"platt_path\": PLATT_FILE,\n",
    "#         \"isotonic_path\": ISO_FILE,\n",
    "#         \"metrics_holdout\": results\n",
    "#     }\n",
    "#     with open(REPORT_FILE, \"w\") as f:\n",
    "#         json.dump(report, f, indent=2)\n",
    "\n",
    "#     return platt, iso, report\n",
    "\n",
    "# def apply_calibrator(probs, method=\"platt\"):\n",
    "#     \"\"\"\n",
    "#     Apply saved calibrator to a numpy array of probs (new/test).\n",
    "#     method: \"platt\" or \"isotonic\"\n",
    "#     \"\"\"\n",
    "#     if method == \"platt\":\n",
    "#         if not os.path.exists(PLATT_FILE):\n",
    "#             raise FileNotFoundError(\"Platt calibrator not found. Run training first.\")\n",
    "#         platt = joblib.load(PLATT_FILE)\n",
    "#         return platt.predict_proba(np.array(probs).reshape(-1, 1))[:, 1]\n",
    "#     elif method == \"isotonic\":\n",
    "#         if not os.path.exists(ISO_FILE):\n",
    "#             raise FileNotFoundError(\"Isotonic calibrator not found. Run training first.\")\n",
    "#         iso = joblib.load(ISO_FILE)\n",
    "#         return iso.predict(np.array(probs))\n",
    "#     else:\n",
    "#         raise ValueError(\"method must be 'platt' or 'isotonic'\")\n",
    "\n",
    "# def summary_print(report):\n",
    "#     print(\"Calibration report summary\")\n",
    "#     print(f\"Samples: {report['n_samples']}, holdout starts at index {report['holdout_start_index']}\")\n",
    "#     for name, m in report[\"metrics_holdout\"].items():\n",
    "#         print(f\"{name.upper():8s}  Brier: {m['brier']:.5f}  LogLoss: {m['logloss']:.5f}  ROC-AUC: {m['roc_auc']:.5f}\")\n",
    "#     print(\"Saved calibrators to:\", report[\"platt_path\"], report[\"isotonic_path\"])\n",
    "#     print(\"Full report JSON:\", REPORT_FILE)\n",
    "\n",
    "\n",
    "# # Load OOF average probs and targets\n",
    "# probs, y, df = load_stack(STACK_PATH)\n",
    "\n",
    "# # Train calibrators and save\n",
    "# platt, iso, report = train_calibrators(probs, y, holdout_frac=0.2)\n",
    "# summary_print(report)\n",
    "\n",
    "# # Example: apply calibrator to the same OOF holdout set to inspect mapping\n",
    "# split_idx = report[\"holdout_start_index\"]\n",
    "# print(\"Holdout class distribution:\")\n",
    "# print(pd.Series(y[split_idx]).value_counts(normalize=True))\n",
    "\n",
    "# raw_auc = roc_auc_score(y, probs)\n",
    "# raw_acc = accuracy_score(y, (probs > 0.5).astype(int))\n",
    "# raw_f1 = f1_score(y, (probs > 0.5).astype(int))\n",
    "# print(f\"Raw OOF - AUC: {raw_auc:.4f}, Acc: {raw_acc:.4f}, F1: {raw_f1:.4f}\")\n",
    "\n",
    "# p_hold = probs[split_idx:]\n",
    "# p_platt = apply_calibrator(p_hold, method=\"platt\")\n",
    "# p_iso = apply_calibrator(p_hold, method=\"isotonic\")\n",
    "\n",
    "# # Save holdout-calibrated probs for inspection\n",
    "# hold_df = pd.DataFrame({\n",
    "#     \"raw_prob\": p_hold,\n",
    "#     \"platt_prob\": p_platt,\n",
    "#     \"isotonic_prob\": p_iso,\n",
    "#     \"target\": y[split_idx:]\n",
    "# })\n",
    "# hold_df.to_csv(os.path.join(OUTPUT_DIR, \"holdout_calibrated_probs.csv\"), index=False)\n",
    "# print(\"Saved holdout calibrated probs to:\", os.path.join(OUTPUT_DIR, \"holdout_calibrated_probs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a5c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use in the backtest:\n",
    "\n",
    "# from calibrate_probs import apply_calibrator\n",
    "\n",
    "# # suppose avg_prob is your averaged ensemble probabilities (numpy array)\n",
    "# calibrated_prob = apply_calibrator(avg_prob, method=\"platt\")  # or \"isotonic\"\n",
    "# signals = generate_signals(calibrated_prob, threshold=chosen_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dad76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ensemble_uncertainty(probs_array):\n",
    "#     \"\"\"\n",
    "#     Compute mean prediction + uncertainty from multiple models.\n",
    "    \n",
    "#     Args:\n",
    "#         probs_array: numpy array of shape (n_models, n_samples)\n",
    "    \n",
    "#     Returns:\n",
    "#         mean_prob: (n_samples,) averaged probability\n",
    "#         uncertainty: (n_samples,) standard deviation across models\n",
    "#     \"\"\"\n",
    "#     mean_prob = np.mean(probs_array, axis=0)\n",
    "#     uncertainty = np.std(probs_array, axis=0)\n",
    "#     return mean_prob, uncertainty\n",
    "\n",
    "\n",
    "# def mc_dropout_predictions(model, X, n_iter=30, batch_size=32):\n",
    "#     \"\"\"\n",
    "#     Monte Carlo Dropout predictions for uncertainty estimation.\n",
    "    \n",
    "#     Args:\n",
    "#         model: tf.keras.Model with Dropout layers\n",
    "#         X: input data\n",
    "#         n_iter: number of stochastic forward passes\n",
    "#         batch_size: batch size\n",
    "    \n",
    "#     Returns:\n",
    "#         probs: numpy array of shape (n_iter, n_samples)\n",
    "#     \"\"\"\n",
    "#     f = tf.function(lambda inp: model(inp, training=True))\n",
    "#     preds = []\n",
    "#     for _ in range(n_iter):\n",
    "#         p = f(X, training=True).numpy().ravel()\n",
    "#         preds.append(p)\n",
    "#     return np.array(preds)\n",
    "\n",
    "\n",
    "# def mc_dropout_uncertainty(model, X, n_iter=30, batch_size=32):\n",
    "#     \"\"\"\n",
    "#     Mean + uncertainty using MC Dropout.\n",
    "#     \"\"\"\n",
    "#     probs = mc_dropout_predictions(model, X, n_iter=n_iter, batch_size=batch_size)\n",
    "#     mean_prob = probs.mean(axis=0)\n",
    "#     uncertainty = probs.std(axis=0)\n",
    "#     return mean_prob, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e1db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose you have probabilities from your 3 base models\n",
    "# prob_lstm = np.load(\"experiments/lstm/lstm_oof.npy\")\n",
    "# prob_gru  = np.load(\"experiments/gru/gru_oof.npy\")\n",
    "# prob_conv = np.load(\"experiments/conv1d/conv1d_oof.npy\")\n",
    "\n",
    "# # Stack them\n",
    "# probs_array = np.vstack([prob_lstm, prob_gru, prob_conv])\n",
    "\n",
    "# mean_prob, uncertainty = ensemble_uncertainty(probs_array)\n",
    "\n",
    "# print(\"First 5 probs:\", mean_prob[:5])\n",
    "# print(\"First 5 uncertainties:\", uncertainty[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
