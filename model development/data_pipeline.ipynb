{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf # Yahoo Finance\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "TICKER = \"BTC-USD\" # a nickname/shorthand for a stock e.g NGN for Nigerian Naira\n",
    "DATA_DIR = \"data/raw\"\n",
    "MANIFEST_FILE = os.path.join(DATA_DIR, \"manifest.json\") # a metadata which describes the data(receipt)\n",
    "START_DATE = \"2015-01-01\"   # earliest Yahoo Finance data for BTC\n",
    "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "INTERVAL = \"1d\"  # daily granularity\n",
    "PROJECT_DIRS = [\n",
    "    \"data/raw\",\n",
    "    \"data/processed\",\n",
    "    \"data/features\",\n",
    "    \"experiments\",\n",
    "    \"models\"\n",
    "]\n",
    "\n",
    "RAW_DIR = \"data/raw\"\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "\n",
    "FEATURES_DIR = \"data/features\"\n",
    "PROCESSED_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_features.parquet\")\n",
    "OUTPUT_FILE = os.path.join(FEATURES_DIR, \"BTC-USD_daily_ml_ready.parquet\")\n",
    "SCALER_FILE = os.path.join(FEATURES_DIR, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eddbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7820\\4268113255.py:16: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(TICKER, start=START_DATE, end=END_DATE, interval=INTERVAL)\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching BTC-USD data from 2015-01-01 to 2025-09-21...\n",
      "Saved data to data/raw\\BTC-USD_1d_2015-01-01_2025-09-21.csv.gz\n",
      "Manifest updated: data/raw\\manifest.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def ensure_dirs():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def load_manifest():\n",
    "    if os.path.exists(MANIFEST_FILE):\n",
    "        with open(MANIFEST_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_manifest(manifest):\n",
    "    with open(MANIFEST_FILE, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "def fetch_btc_data():\n",
    "    print(f\"Fetching {TICKER} data from {START_DATE} to {END_DATE}...\")\n",
    "    df = yf.download(TICKER, start=START_DATE, end=END_DATE, interval=INTERVAL)\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def save_data(df):\n",
    "    fname = f\"{TICKER}_{INTERVAL}_{START_DATE}_{END_DATE}.csv.gz\" # a csv file which has been compressed\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    df.to_csv(fpath, index=False, compression=\"gzip\")\n",
    "    print(f\"Saved data to {fpath}\")\n",
    "    return fpath\n",
    "\n",
    "def setup_directories():\n",
    "    for d in PROJECT_DIRS:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def scrape_data():\n",
    "    ensure_dirs()\n",
    "    setup_directories()\n",
    "    manifest = load_manifest()\n",
    "    df = fetch_btc_data()\n",
    "    fpath = save_data(df)\n",
    "\n",
    "    # Update manifest\n",
    "    manifest_entry = {\n",
    "        \"source\": \"yfinance\",\n",
    "        \"ticker\": TICKER,\n",
    "        \"interval\": INTERVAL,\n",
    "        \"rows\": len(df),\n",
    "        \"start\": str(df[\"Date\"].iloc[0]),\n",
    "        \"end\": str(df[\"Date\"].iloc[-1]),\n",
    "        \"file\": fpath,\n",
    "        \"downloaded_at\": datetime.now().isoformat()\n",
    "    }\n",
    "    manifest[f\"{TICKER}_{INTERVAL}\"] = manifest_entry\n",
    "    save_manifest(manifest)\n",
    "\n",
    "    print(\"Manifest updated:\", MANIFEST_FILE)\n",
    "\n",
    "scrape_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf81892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to data/processed\\BTC-USD_daily_clean.parquet\n",
      "Rows: 3916, Date range: 2015-01-01 00:00:00+00:00 → 2025-09-20 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "def clean_btc_data(raw_file):\n",
    "    # Load raw data\n",
    "    df = pd.read_csv(raw_file, parse_dates=[\"Date\"])\n",
    "    \n",
    "    # Standardize column names\n",
    "    df = df.rename(columns={\n",
    "        \"Date\": \"timestamp\",\n",
    "        \"Open\": \"open\",\n",
    "        \"High\": \"high\",\n",
    "        \"Low\": \"low\",\n",
    "        \"Close\": \"close\",\n",
    "        \"Adj Close\": \"adj_close\",\n",
    "        \"Volume\": \"volume\"\n",
    "    })\n",
    "    \n",
    "    # Drop Adj Close (redundant for crypto)\n",
    "    df = df.drop(columns=[\"adj_close\"], errors=\"ignore\")\n",
    "\n",
    "    # Ensure sorted\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Align to daily frequency (UTC)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    full_range = pd.date_range(\n",
    "        start=df[\"timestamp\"].min(), \n",
    "        end=df[\"timestamp\"].max(), \n",
    "        freq=\"1D\", \n",
    "        tz=\"UTC\"\n",
    "    )\n",
    "    df = df.set_index(\"timestamp\").reindex(full_range)\n",
    "\n",
    "    # Fill missing values\n",
    "    df[\"open\"] = df[\"open\"].ffill()\n",
    "    df[\"high\"] = df[\"high\"].ffill()\n",
    "    df[\"low\"] = df[\"low\"].ffill()\n",
    "    df[\"close\"] = df[\"close\"].ffill()\n",
    "    df[\"volume\"] = df[\"volume\"].fillna(0)\n",
    "\n",
    "    # Reset index\n",
    "    df = df.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "\n",
    "    # Remove duplicates if any\n",
    "    df = df.drop_duplicates(subset=[\"timestamp\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data():\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "    # Find latest raw file\n",
    "    raw_files = [f for f in os.listdir(RAW_DIR) if f.endswith(\".csv.gz\")]\n",
    "    if not raw_files:\n",
    "        raise FileNotFoundError(\"No raw data files found in data/raw/\")\n",
    "    raw_file = os.path.join(RAW_DIR, sorted(raw_files)[-1])\n",
    "\n",
    "    # Clean\n",
    "    cleaned_df = clean_btc_data(raw_file)\n",
    "\n",
    "    # Save\n",
    "    processed_file = os.path.join(PROCESSED_DIR, \"BTC-USD_daily_clean.parquet\")\n",
    "    cleaned_df.to_parquet(processed_file, index=False)\n",
    "    print(f\"Saved cleaned data to {processed_file}\")\n",
    "    print(f\"Rows: {len(cleaned_df)}, Date range: {cleaned_df['timestamp'].min()} → {cleaned_df['timestamp'].max()}\")\n",
    "\n",
    "clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe252eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to data/features\\BTC-USD_daily_features.parquet\n",
      "Final shape: (3717, 33), Columns: 33\n"
     ]
    }
   ],
   "source": [
    "def add_indicators(df):\n",
    "    \"\"\"Compute technical indicators and returns/volatility features.\"\"\"\n",
    "\n",
    "    # --- Moving Averages ---\n",
    "    for win in [5, 10, 21, 50, 200]:\n",
    "        df[f\"sma_{win}\"] = df[\"close\"].rolling(win).mean()\n",
    "        df[f\"ema_{win}\"] = df[\"close\"].ewm(span=win, adjust=False).mean()\n",
    "\n",
    "    # --- Momentum / Returns ---\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "    df[\"pct_change\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # Multi-horizon returns\n",
    "    for h in [3, 7, 14, 30]:\n",
    "        df[f\"return_{h}d\"] = df[\"close\"].pct_change(h)\n",
    "\n",
    "    # --- Volatility ---\n",
    "    for win in [5, 10, 21]:\n",
    "        df[f\"volatility_{win}d\"] = df[\"log_return\"].rolling(win).std()\n",
    "\n",
    "    # --- RSI (14-day default) ---\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "    roll_up = pd.Series(gain).rolling(14).mean()\n",
    "    roll_down = pd.Series(loss).rolling(14).mean()\n",
    "    rs = roll_up / (roll_down + 1e-9)  # avoid div/0\n",
    "    df[\"rsi_14\"] = 100.0 - (100.0 / (1.0 + rs))\n",
    "\n",
    "    # --- Bollinger Bands (20, 2) ---\n",
    "    rolling_mean = df[\"close\"].rolling(20).mean()\n",
    "    rolling_std = df[\"close\"].rolling(20).std()\n",
    "    df[\"bollinger_mid\"] = rolling_mean\n",
    "    df[\"bollinger_up\"] = rolling_mean + (rolling_std * 2)\n",
    "    df[\"bollinger_down\"] = rolling_mean - (rolling_std * 2)\n",
    "\n",
    "    # --- MACD (12,26,9) ---\n",
    "    ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"macd\"] = ema12 - ema26\n",
    "    df[\"macd_signal\"] = df[\"macd\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # --- On-Balance Volume (OBV) ---\n",
    "    df[\"obv\"] = (np.sign(df[\"close\"].diff()) * df[\"volume\"]).fillna(0).cumsum()\n",
    "\n",
    "    # --- VWAP (Volume Weighted Average Price) ---\n",
    "    df[\"vwap\"] = (df[\"close\"] * df[\"volume\"]).cumsum() / (df[\"volume\"].cumsum() + 1e-9)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_features():\n",
    "    os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "    # Load cleaned data\n",
    "    processed_file = os.path.join(PROCESSED_DIR, \"BTC-USD_daily_clean.parquet\") # unlike row based csv, parquet is column based\n",
    "    if not os.path.exists(processed_file):\n",
    "        raise FileNotFoundError(\"Run clean_data first to generate processed data\")\n",
    "    \n",
    "    df = pd.read_parquet(processed_file)\n",
    "\n",
    "    numeric_cols = [\"close\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Add features\n",
    "    df = add_indicators(df)\n",
    "\n",
    "    # Drop initial rows with NaNs from rolling windows\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Save\n",
    "    features_file = os.path.join(FEATURES_DIR, \"BTC-USD_daily_features.parquet\")\n",
    "    df.to_parquet(features_file, index=False)\n",
    "    print(f\"Saved features to {features_file}\")\n",
    "    print(f\"Final shape: {df.shape}, Columns: {len(df.columns)}\")\n",
    "\n",
    "build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b03c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ML-ready features to data/features\\BTC-USD_daily_ml_ready.parquet\n",
      "Saved scaler to data/features\\scaler.pkl\n",
      "Shape: (3710, 63), Features: 63\n"
     ]
    }
   ],
   "source": [
    "def add_lagged_features(df, cols, lags=[1, 2, 3, 5, 7]):\n",
    "    \"\"\"Add lagged versions of selected columns.\"\"\"\n",
    "    for col in cols:\n",
    "        for lag in lags:\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def scale_features(df, exclude_cols):\n",
    "    \"\"\"Scale numeric features except excluded ones.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    cols_to_scale = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[cols_to_scale] = scaler.fit_transform(df_scaled[cols_to_scale])\n",
    "\n",
    "    return df_scaled, scaler, cols_to_scale\n",
    "\n",
    "def features_engineering():\n",
    "    if not os.path.exists(PROCESSED_FILE):\n",
    "        raise FileNotFoundError(\"Run build_features.py first to generate base features\")\n",
    "\n",
    "    df = pd.read_parquet(PROCESSED_FILE)\n",
    "\n",
    "    # --- Step 1: Lagged features ---\n",
    "    lag_cols = [\"close\", \"volume\", \"sma_10\", \"ema_10\", \"rsi_14\", \"macd\"]\n",
    "    df = add_lagged_features(df, lag_cols)\n",
    "\n",
    "    # Drop NaN rows caused by lagging\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # --- Step 2: Scaling ---\n",
    "    exclude_cols = [\"timestamp\"]  # keep time intact\n",
    "    df_scaled, scaler, scaled_cols = scale_features(df, exclude_cols)\n",
    "\n",
    "    # --- Save outputs ---\n",
    "    os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "    df_scaled.to_parquet(OUTPUT_FILE, index=False)\n",
    "    joblib.dump({\"scaler\": scaler, \"scaled_cols\": scaled_cols}, SCALER_FILE)\n",
    "\n",
    "    print(f\"Saved ML-ready features to {OUTPUT_FILE}\")\n",
    "    print(f\"Saved scaler to {SCALER_FILE}\")\n",
    "    print(f\"Shape: {df_scaled.shape}, Features: {len(df_scaled.columns)}\")\n",
    "\n",
    "features_engineering()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
